{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import random\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from imblearn.combine import SMOTEENN\n",
    "import keras\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Dropout, Input, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data to Panda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/clean_testing_Candy.csv', index_col=0, low_memory=False)\n",
    "del df['treatment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t0_geoip_metro_code_others_level</th>\n",
       "      <th>t0_geoip_metro_code_MC_501</th>\n",
       "      <th>t0_geoip_metro_code_MC_504</th>\n",
       "      <th>t0_geoip_metro_code_MC_506</th>\n",
       "      <th>t0_geoip_metro_code_MC_511</th>\n",
       "      <th>t0_geoip_metro_code_MC_524</th>\n",
       "      <th>t0_geoip_metro_code_MC_602</th>\n",
       "      <th>t0_geoip_metro_code_MC_618</th>\n",
       "      <th>t0_geoip_metro_code_MC_623</th>\n",
       "      <th>t0_geoip_metro_code_MC_803</th>\n",
       "      <th>...</th>\n",
       "      <th>median_hh_inc_NA</th>\n",
       "      <th>mean_hh_inc_NA</th>\n",
       "      <th>owner_occupied_hh_NA</th>\n",
       "      <th>pct_native_born_NA</th>\n",
       "      <th>pct_white_NA</th>\n",
       "      <th>pct_men_in_labor_force_NA</th>\n",
       "      <th>pct_women_in_labor_force_NA</th>\n",
       "      <th>pct_25_up_NA</th>\n",
       "      <th>ppl_per_hh_NA</th>\n",
       "      <th>pct_owner_occupied_hh_NA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 321 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   t0_geoip_metro_code_others_level  t0_geoip_metro_code_MC_501  \\\n",
       "1                                 0                           0   \n",
       "2                                 0                           0   \n",
       "3                                 0                           0   \n",
       "4                                 0                           0   \n",
       "5                                 1                           0   \n",
       "\n",
       "   t0_geoip_metro_code_MC_504  t0_geoip_metro_code_MC_506  \\\n",
       "1                           0                           0   \n",
       "2                           0                           0   \n",
       "3                           0                           0   \n",
       "4                           0                           0   \n",
       "5                           0                           0   \n",
       "\n",
       "   t0_geoip_metro_code_MC_511  t0_geoip_metro_code_MC_524  \\\n",
       "1                           0                           0   \n",
       "2                           0                           0   \n",
       "3                           0                           0   \n",
       "4                           0                           0   \n",
       "5                           0                           0   \n",
       "\n",
       "   t0_geoip_metro_code_MC_602  t0_geoip_metro_code_MC_618  \\\n",
       "1                           0                           0   \n",
       "2                           0                           1   \n",
       "3                           0                           0   \n",
       "4                           1                           0   \n",
       "5                           0                           0   \n",
       "\n",
       "   t0_geoip_metro_code_MC_623  t0_geoip_metro_code_MC_803  \\\n",
       "1                           0                           0   \n",
       "2                           0                           0   \n",
       "3                           0                           0   \n",
       "4                           0                           0   \n",
       "5                           0                           0   \n",
       "\n",
       "             ...             median_hh_inc_NA  mean_hh_inc_NA  \\\n",
       "1            ...                            1               1   \n",
       "2            ...                            0               0   \n",
       "3            ...                            1               1   \n",
       "4            ...                            0               0   \n",
       "5            ...                            0               0   \n",
       "\n",
       "   owner_occupied_hh_NA  pct_native_born_NA  pct_white_NA  \\\n",
       "1                     1                   1             1   \n",
       "2                     0                   0             0   \n",
       "3                     1                   1             1   \n",
       "4                     0                   0             0   \n",
       "5                     0                   0             0   \n",
       "\n",
       "   pct_men_in_labor_force_NA  pct_women_in_labor_force_NA  pct_25_up_NA  \\\n",
       "1                          1                            1             1   \n",
       "2                          0                            0             0   \n",
       "3                          1                            1             1   \n",
       "4                          0                            0             0   \n",
       "5                          0                            0             0   \n",
       "\n",
       "   ppl_per_hh_NA  pct_owner_occupied_hh_NA  \n",
       "1              1                         1  \n",
       "2              0                         0  \n",
       "3              1                         1  \n",
       "4              0                         0  \n",
       "5              0                         0  \n",
       "\n",
       "[5 rows x 321 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling the Non-spenders to 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8a48c86080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nonspenders = df[df.out_ltv==0]\n",
    "\n",
    "nonspender_sampleDF = nonspenders.sample(n=200000, random_state=40)\n",
    "\n",
    "spenderDF = df[df.out_ltv>0]\n",
    "\n",
    "new_df = pd.concat([nonspender_sampleDF, spenderDF])\n",
    "\n",
    "# Plot the LTV distribution for spenders only\n",
    "new_df['out_ltv'][new_df['out_ltv']>0].hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array(new_df.drop(['out_ltv'], axis=1))\n",
    "y = np.array(new_df['out_ltv'])\n",
    "# y = y.values.reshape((y.shape[0],1))\n",
    "\n",
    "seed=7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# split data into training set and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the training\n",
    "norm_train, norm = normalize(X_train, axis=0, return_norm=True)\n",
    "norm_test = X_test/norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.save('Data/norm_train.npy', norm_train)\n",
    "# np.save('Data/norm_test.npy', norm_test)\n",
    "# np.save('Data/y_train.npy', y_train)\n",
    "# np.save('Data/y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance the data - by oversampling with replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_label = y_train>0\n",
    "train_label = train_label.astype(np.bool)\n",
    "test_label = y_test>0\n",
    "test_label = test_label.astype(np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed=7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# randomly select as many buyers as non_buyers from training data\n",
    "sample_index = np.random.choice(len(norm_train[train_label]), size=160014)\n",
    "oversampled_norm_train = np.concatenate((norm_train[train_label][sample_index], norm_train[y_train==0]), axis=0)\n",
    "oversampled_y_train = np.concatenate((y_train[train_label][sample_index], y_train[y_train==0]), axis=0)\n",
    "\n",
    "# update training label\n",
    "train_label = oversampled_y_train>0\n",
    "train_label = train_label.astype(np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({True: 160014, False: 160014})\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAD8CAYAAAChHgmuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8a48c86390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(Counter(train_label))\n",
    "sns.countplot(train_label);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.save('Data/oversampled_X_train', oversampled_norm_train)\n",
    "# np.save('Data/overssampled_y_train', oversampled_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining undersampling and oversampling using SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before smoting, feature selection needs to be done. Here we use Autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 135251 samples, validate on 33813 samples\n",
      "Epoch 1/50\n",
      "135251/135251 [==============================] - 3s - loss: 3.9387e-06 - rmse_: 0.0020 - val_loss: 3.5722e-06 - val_rmse_: 0.0019\n",
      "Epoch 2/50\n",
      "135251/135251 [==============================] - 3s - loss: 3.3609e-06 - rmse_: 0.0018 - val_loss: 3.3659e-06 - val_rmse_: 0.0018\n",
      "Epoch 3/50\n",
      "135251/135251 [==============================] - 3s - loss: 3.2059e-06 - rmse_: 0.0018 - val_loss: 3.1390e-06 - val_rmse_: 0.0018\n",
      "Epoch 4/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.8532e-06 - rmse_: 0.0017 - val_loss: 2.7584e-06 - val_rmse_: 0.0017\n",
      "Epoch 5/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.6143e-06 - rmse_: 0.0016 - val_loss: 2.5891e-06 - val_rmse_: 0.0016\n",
      "Epoch 6/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.4835e-06 - rmse_: 0.0016 - val_loss: 2.4746e-06 - val_rmse_: 0.0016\n",
      "Epoch 7/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3928e-06 - rmse_: 0.0015 - val_loss: 2.4084e-06 - val_rmse_: 0.0015\n",
      "Epoch 8/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3414e-06 - rmse_: 0.0015 - val_loss: 2.3818e-06 - val_rmse_: 0.0015\n",
      "Epoch 9/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3336e-06 - rmse_: 0.0015 - val_loss: 2.3764e-06 - val_rmse_: 0.0015\n",
      "Epoch 10/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3274e-06 - rmse_: 0.0015 - val_loss: 2.3691e-06 - val_rmse_: 0.0015\n",
      "Epoch 11/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3220e-06 - rmse_: 0.0015 - val_loss: 2.3652e-06 - val_rmse_: 0.0015\n",
      "Epoch 12/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3169e-06 - rmse_: 0.0015 - val_loss: 2.3594e-06 - val_rmse_: 0.0015\n",
      "Epoch 13/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3092e-06 - rmse_: 0.0015 - val_loss: 2.3547e-06 - val_rmse_: 0.0015\n",
      "Epoch 14/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3083e-06 - rmse_: 0.0015 - val_loss: 2.3540e-06 - val_rmse_: 0.0015\n",
      "Epoch 15/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3075e-06 - rmse_: 0.0015 - val_loss: 2.3529e-06 - val_rmse_: 0.0015\n",
      "Epoch 16/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3066e-06 - rmse_: 0.0015 - val_loss: 2.3521e-06 - val_rmse_: 0.0015\n",
      "Epoch 17/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3057e-06 - rmse_: 0.0015 - val_loss: 2.3515e-06 - val_rmse_: 0.0015\n",
      "Epoch 18/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3041e-06 - rmse_: 0.0015 - val_loss: 2.3502e-06 - val_rmse_: 0.0015\n",
      "Epoch 19/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3039e-06 - rmse_: 0.0015 - val_loss: 2.3500e-06 - val_rmse_: 0.0015\n",
      "Epoch 20/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3038e-06 - rmse_: 0.0015 - val_loss: 2.3499e-06 - val_rmse_: 0.0015\n",
      "Epoch 21/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3036e-06 - rmse_: 0.0015 - val_loss: 2.3497e-06 - val_rmse_: 0.0015\n",
      "Epoch 22/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3034e-06 - rmse_: 0.0015 - val_loss: 2.3495e-06 - val_rmse_: 0.0015\n",
      "Epoch 23/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3031e-06 - rmse_: 0.0015 - val_loss: 2.3492e-06 - val_rmse_: 0.0015\n",
      "Epoch 24/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3030e-06 - rmse_: 0.0015 - val_loss: 2.3492e-06 - val_rmse_: 0.0015\n",
      "Epoch 25/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3030e-06 - rmse_: 0.0015 - val_loss: 2.3492e-06 - val_rmse_: 0.0015\n",
      "Epoch 26/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3029e-06 - rmse_: 0.0015 - val_loss: 2.3491e-06 - val_rmse_: 0.0015\n",
      "Epoch 27/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3029e-06 - rmse_: 0.0015 - val_loss: 2.3491e-06 - val_rmse_: 0.0015\n",
      "Epoch 28/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3028e-06 - rmse_: 0.0015 - val_loss: 2.3491e-06 - val_rmse_: 0.0015\n",
      "Epoch 29/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3028e-06 - rmse_: 0.0015 - val_loss: 2.3491e-06 - val_rmse_: 0.0015\n",
      "Epoch 30/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3028e-06 - rmse_: 0.0015 - val_loss: 2.3490e-06 - val_rmse_: 0.0015\n",
      "Epoch 31/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3028e-06 - rmse_: 0.0015 - val_loss: 2.3490e-06 - val_rmse_: 0.0015\n",
      "Epoch 32/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3028e-06 - rmse_: 0.0015 - val_loss: 2.3490e-06 - val_rmse_: 0.0015\n",
      "Epoch 33/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3028e-06 - rmse_: 0.0015 - val_loss: 2.3490e-06 - val_rmse_: 0.0015\n",
      "Epoch 34/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3028e-06 - rmse_: 0.0015 - val_loss: 2.3490e-06 - val_rmse_: 0.0015\n",
      "Epoch 35/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3028e-06 - rmse_: 0.0015 - val_loss: 2.3490e-06 - val_rmse_: 0.0015\n",
      "Epoch 36/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3028e-06 - rmse_: 0.0015 - val_loss: 2.3490e-06 - val_rmse_: 0.0015\n",
      "Epoch 37/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3028e-06 - rmse_: 0.0015 - val_loss: 2.3490e-06 - val_rmse_: 0.0015\n",
      "Epoch 38/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3028e-06 - rmse_: 0.0015 - val_loss: 2.3490e-06 - val_rmse_: 0.0015\n",
      "Epoch 39/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3028e-06 - rmse_: 0.0015 - val_loss: 2.3490e-06 - val_rmse_: 0.0015\n",
      "Epoch 40/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3028e-06 - rmse_: 0.0015 - val_loss: 2.3490e-06 - val_rmse_: 0.0015\n",
      "Epoch 41/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3028e-06 - rmse_: 0.0015 - val_loss: 2.3490e-06 - val_rmse_: 0.0015\n",
      "Epoch 42/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3028e-06 - rmse_: 0.0015 - val_loss: 2.3490e-06 - val_rmse_: 0.0015\n",
      "Epoch 43/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3028e-06 - rmse_: 0.0015 - val_loss: 2.3490e-06 - val_rmse_: 0.0015\n",
      "Epoch 44/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3028e-06 - rmse_: 0.0015 - val_loss: 2.3490e-06 - val_rmse_: 0.0015\n",
      "Epoch 45/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3028e-06 - rmse_: 0.0015 - val_loss: 2.3490e-06 - val_rmse_: 0.0015\n",
      "Epoch 46/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3028e-06 - rmse_: 0.0015 - val_loss: 2.3490e-06 - val_rmse_: 0.0015\n",
      "Epoch 47/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3028e-06 - rmse_: 0.0015 - val_loss: 2.3490e-06 - val_rmse_: 0.0015\n",
      "Epoch 48/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3028e-06 - rmse_: 0.0015 - val_loss: 2.3490e-06 - val_rmse_: 0.0015\n",
      "Epoch 49/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3028e-06 - rmse_: 0.0015 - val_loss: 2.3490e-06 - val_rmse_: 0.0015\n",
      "Epoch 50/50\n",
      "135251/135251 [==============================] - 3s - loss: 2.3028e-06 - rmse_: 0.0015 - val_loss: 2.3490e-06 - val_rmse_: 0.0015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8aacd2b6d8>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rmse_(y_true, y_pred):\n",
    "    return (K.mean((((y_pred-y_true))**2)))**(1/2)\n",
    "\n",
    "# Build a FNN model with 2 hidden layers that maps input data to itself\n",
    "input_data = Input(shape=(norm_train.shape[1],))\n",
    "encode = Dense(128, activation='relu')(input_data)\n",
    "encode2 = Dense(50, activation='relu')(encode)\n",
    "decode = Dense(norm_train.shape[1])(encode2)\n",
    "\n",
    "# This model uses all the layers. We will later use this model to evaluate\n",
    "decode_model = Model(input_data, decode)\n",
    "\n",
    "# This model uses the 2nd Dense layer to map from Dimension of input to Dimension of 50\n",
    "encode_model = Model(input_data, encode2)\n",
    "\n",
    "loss='mean_squared_error'\n",
    "optimizer=Adam(lr=0.001)\n",
    "metric=[rmse_]\n",
    "\n",
    "decode_model.compile(loss=loss, \n",
    "              optimizer=optimizer, \n",
    "              metrics=metric)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                  factor=0.2, \n",
    "                  patience=5,   \n",
    "                  epsilon=0.0001,  \n",
    "                  min_lr=0)\n",
    "\n",
    "modelchecker = ModelCheckpoint('autoencoder.h5', save_best_only=True)\n",
    "\n",
    "earlystopper = EarlyStopping(patience=12, mode='min', verbose=1)\n",
    "\n",
    "decode_model.fit(norm_train, \n",
    "              norm_train,\n",
    "              batch_size=300, \n",
    "              epochs=50, \n",
    "              validation_split=0.2, \n",
    "              verbose=1, \n",
    "              callbacks=[reduce_lr, earlystopper, modelchecker])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42208/42266 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.2881733567348365e-06, 0.0015084358026221415]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dimension reduction for training set\n",
    "reduced_data_train = encode_model.predict(norm_train)\n",
    "\n",
    "# dimension reduction for testing set\n",
    "reduced_data_test = encode_model.predict(norm_test)\n",
    "\n",
    "# Evaluate our model by projecting test data to itself\n",
    "decode_model.evaluate(norm_test, norm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.save('Data/autoencoded_data_train.npy', reduced_data_train)\n",
    "# np.save('Data/autoencoded_data_test.npy', reduced_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now we can smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0.0: 26636, 1.0: 1542})\n",
      "Counter({0.0: 26655, 1.0: 1523})\n",
      "Counter({0.0: 26697, 1.0: 1480})\n",
      "Counter({0.0: 26651, 1.0: 1526})\n",
      "Counter({0.0: 26690, 1.0: 1487})\n",
      "Counter({0.0: 26685, 1.0: 1492})\n"
     ]
    }
   ],
   "source": [
    "# Create new label that indicates spenders as True, non-spenders as False\n",
    "label = y_train >0\n",
    "\n",
    "side = np.hstack((reduced_data_train, y_train.reshape((y_train.shape[0],1))))\n",
    "train = np.hstack((side, label.reshape((label.shape[0],1))))\n",
    "\n",
    "# split the entire training set to 6 small subsets to avoid out of memory error\n",
    "split_list = np.array_split(train, 6)\n",
    "\n",
    "for array in split_list:\n",
    "    print(Counter(array[:,-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Smote can generate noisy data by interpolating new points between marginal outliers and inliers. This issue can be solved by cleaning the resulted space obtained after over-sampling. We choose Edited nearest-neighbours because it clears more noisy data than SmoteTomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make functions that will smote the data for us\n",
    "def resample(X, y):\n",
    "    sm = SMOTEENN()\n",
    "    X_resampled, y_resampled = sm.fit_sample(X, y)\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "def resample_the_list(list_array):\n",
    "    list_X_resampled = []\n",
    "    list_y_resampled = []\n",
    "    for array in list_array:\n",
    "        X_resampled, y_resampled, = resample(array[:,:-1], array[:, -1].astype(np.bool))\n",
    "        list_X_resampled.append(X_resampled)\n",
    "        list_y_resampled.append(y_resampled)\n",
    "    return list_X_resampled, list_y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_X_resampled, list_y_resampled = resample_the_list(split_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we will only save the X_resampled because y_resampled can be easily created by passing a booling function\n",
    "final_train = np.concatenate(list_X_resampled, axis=0)\n",
    "# np.save('Data/smoted_train_X_and_y.npy', final_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
