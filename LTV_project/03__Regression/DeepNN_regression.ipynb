{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rstudio/efs/anaconda3/envs/dl/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import random\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.manifold import TSNE\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "import keras\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Dropout, Input, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmse(targets, predictions):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n",
    "def rmse_(y_true, y_pred):\n",
    "    return (K.mean((((y_pred-y_true))**2)))**(1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load oversampled X\n",
    "oversampled_X_train = np.load('../Data/oversampled_X_train.npy')\n",
    "# Load oversampled y\n",
    "oversampled_y_train = np.load('../Data/overssampled_y_train.npy')\n",
    "# load smoted_data\n",
    "smoted_X_y = np.load('../Data/smoted_train_X_and_y.npy')\n",
    "# load test set\n",
    "norm_test_x = np.load('../Data/norm_test.npy')\n",
    "dim_reduced_test_x = np.load('../Data/autoencoded_data_test.npy')\n",
    "y_test = np.load('../Data/y_test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Classfication model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp_classification_oversample = keras.models.load_model('../02__Classification/mlp_classification_oversample.h5')\n",
    "mlp_classification_smote = keras.models.load_model('../02__Classification/mlp_classification_smote.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Because our classfication model has a higher false positive rate, we want to select some non-buyers who are very similar to buyers, (i.e. non-spenders who were predicted of being 45%-50%(threshold) of being a buyer) and combine these them as our new traning data for regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_sim_buyers(data, label, model):\n",
    "    non_buyers = data[label==0]\n",
    "    probability = model.predict_proba(non_buyers)\n",
    "    index = np.where(np.logical_and(probability>=0.45, probability<0.5))[0]\n",
    "    return non_buyers[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### find similar buyers for oversampled data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159968/160014 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "# we will add these non_buyers to buyers\n",
    "sim_buyers_x = find_sim_buyers(oversampled_X_train, oversampled_y_train, mlp_classification_oversample)\n",
    "sim_buyers_y = np.zeros((len(sim_buyers_x),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select buyers only\n",
    "buyer_index = oversampled_y_train>0\n",
    "\n",
    "oversampled_X_regression = oversampled_X_train[buyer_index]\n",
    "oversampled_y_regression = oversampled_y_train[buyer_index]\n",
    "oversampled_y_regression = oversampled_y_regression.reshape((oversampled_y_regression.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oversampled_X_regression = np.concatenate((oversampled_X_regression, sim_buyers_x))\n",
    "oversampled_y_regression = np.vstack((oversampled_y_regression,sim_buyers_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQ5LU7X8B14FfasyTe2MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0c54061860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# see how buyers' ltv distributed\n",
    "plt.hist(oversampled_y_regression);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now lets find similar buyers for smote data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smote_x = smoted_X_y[:,:-1]\n",
    "smote_y = smoted_X_y[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158112/160014 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "sim_buyers_xsmote = find_sim_buyers(smote_x, smote_y, mlp_classification_smote)\n",
    "sim_buyers_ysmote = np.zeros((len(sim_buyers_xsmote),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select buyers only\n",
    "buyer_index2 = smote_y>0\n",
    "\n",
    "xsmote_regression = smote_x[buyer_index2]\n",
    "ysmote_regression = smote_y[buyer_index2]\n",
    "ysmote_regression = ysmote_regression.reshape((ysmote_regression.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xsmote_regression = np.concatenate((xsmote_regression, sim_buyers_xsmote))\n",
    "ysmote_regression = np.vstack((ysmote_regression,sim_buyers_ysmote))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling w/ oversampled data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(learn_rate=0.01, decay=0.01 ,init='normal', drop_out=0.2, loss='mse'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=240, kernel_initializer=init, input_shape=(oversampled_X_regression.shape[1],)))\n",
    "    model.add(Activation(activation='relu'))\n",
    "    model.add(Dropout(drop_out))\n",
    "    model.add(Dense(units=120, kernel_initializer=init))\n",
    "    model.add(Activation(activation='relu'))\n",
    "    model.add(Dropout(drop_out))\n",
    "    model.add(Dense(units=60, kernel_initializer=init))\n",
    "    model.add(Activation(activation='relu'))\n",
    "    model.add(Dropout(drop_out))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss=loss, \n",
    "              optimizer=Adam(lr=learn_rate, decay=decay), \n",
    "              metrics=[rmse_])\n",
    "    return model\n",
    "\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_30 (Dense)             (None, 240)               77040     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 240)               0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 240)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 120)               28920     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 60)                7260      \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 1)                 61        \n",
      "=================================================================\n",
      "Total params: 113,281\n",
      "Trainable params: 113,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                  factor=0.2, \n",
    "                  patience=5,   \n",
    "                  epsilon=0.0001,  \n",
    "                  min_lr=0)\n",
    "modelchecker = ModelCheckpoint('mlp_model_OS.h5', save_best_only=True)\n",
    "\n",
    "earlystopper = EarlyStopping(patience=12, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 112247 samples, validate on 55286 samples\n",
      "Epoch 1/1000\n",
      "112247/112247 [==============================] - 3s - loss: 521.3481 - rmse_: 22.7732 - val_loss: 468.3660 - val_rmse_: 21.5134\n",
      "Epoch 2/1000\n",
      "112247/112247 [==============================] - 3s - loss: 504.4369 - rmse_: 22.4120 - val_loss: 460.2331 - val_rmse_: 21.2940\n",
      "Epoch 3/1000\n",
      "112247/112247 [==============================] - 3s - loss: 501.7625 - rmse_: 22.3479 - val_loss: 454.6955 - val_rmse_: 21.1173\n",
      "Epoch 4/1000\n",
      "112247/112247 [==============================] - 3s - loss: 499.8208 - rmse_: 22.3150 - val_loss: 455.7147 - val_rmse_: 21.1728\n",
      "Epoch 5/1000\n",
      "112247/112247 [==============================] - 3s - loss: 498.7808 - rmse_: 22.2838 - val_loss: 458.0826 - val_rmse_: 21.2554\n",
      "Epoch 6/1000\n",
      "112247/112247 [==============================] - 3s - loss: 497.2107 - rmse_: 22.2460 - val_loss: 455.2678 - val_rmse_: 21.1750\n",
      "Epoch 7/1000\n",
      "112247/112247 [==============================] - 3s - loss: 496.2452 - rmse_: 22.2303 - val_loss: 451.4779 - val_rmse_: 21.0532\n",
      "Epoch 8/1000\n",
      "112247/112247 [==============================] - 3s - loss: 494.7142 - rmse_: 22.1941 - val_loss: 452.3182 - val_rmse_: 21.0933\n",
      "Epoch 9/1000\n",
      "112247/112247 [==============================] - 3s - loss: 493.8753 - rmse_: 22.1755 - val_loss: 455.0102 - val_rmse_: 21.1853\n",
      "Epoch 10/1000\n",
      "112247/112247 [==============================] - 3s - loss: 492.7426 - rmse_: 22.1487 - val_loss: 449.3607 - val_rmse_: 21.0076\n",
      "Epoch 11/1000\n",
      "112247/112247 [==============================] - 3s - loss: 491.7629 - rmse_: 22.1293 - val_loss: 449.1680 - val_rmse_: 21.0120\n",
      "Epoch 12/1000\n",
      "112247/112247 [==============================] - 3s - loss: 490.8298 - rmse_: 22.1068 - val_loss: 448.5456 - val_rmse_: 20.9984\n",
      "Epoch 13/1000\n",
      "112247/112247 [==============================] - 3s - loss: 489.4593 - rmse_: 22.0793 - val_loss: 448.7146 - val_rmse_: 21.0127\n",
      "Epoch 14/1000\n",
      "112247/112247 [==============================] - 3s - loss: 488.4939 - rmse_: 22.0490 - val_loss: 447.5308 - val_rmse_: 20.9796\n",
      "Epoch 15/1000\n",
      "112247/112247 [==============================] - 3s - loss: 487.4945 - rmse_: 22.0286 - val_loss: 446.4057 - val_rmse_: 20.9487\n",
      "Epoch 16/1000\n",
      "112247/112247 [==============================] - 3s - loss: 487.1701 - rmse_: 22.0243 - val_loss: 445.5968 - val_rmse_: 20.9266\n",
      "Epoch 17/1000\n",
      "112247/112247 [==============================] - 3s - loss: 486.1259 - rmse_: 22.0020 - val_loss: 443.8053 - val_rmse_: 20.8658\n",
      "Epoch 18/1000\n",
      "112247/112247 [==============================] - 3s - loss: 484.6939 - rmse_: 21.9729 - val_loss: 445.2283 - val_rmse_: 20.9294\n",
      "Epoch 19/1000\n",
      "112247/112247 [==============================] - 3s - loss: 484.5394 - rmse_: 21.9627 - val_loss: 443.6132 - val_rmse_: 20.8793\n",
      "Epoch 20/1000\n",
      "112247/112247 [==============================] - 3s - loss: 483.6518 - rmse_: 21.9435 - val_loss: 445.9070 - val_rmse_: 20.9628\n",
      "Epoch 21/1000\n",
      "112247/112247 [==============================] - 3s - loss: 483.4500 - rmse_: 21.9454 - val_loss: 443.7097 - val_rmse_: 20.8973\n",
      "Epoch 22/1000\n",
      "112247/112247 [==============================] - 3s - loss: 482.3922 - rmse_: 21.9201 - val_loss: 442.5213 - val_rmse_: 20.8606\n",
      "Epoch 23/1000\n",
      "112247/112247 [==============================] - 3s - loss: 481.7273 - rmse_: 21.8979 - val_loss: 441.6866 - val_rmse_: 20.8381\n",
      "Epoch 24/1000\n",
      "112247/112247 [==============================] - 3s - loss: 481.1716 - rmse_: 21.8919 - val_loss: 442.6576 - val_rmse_: 20.8771\n",
      "Epoch 25/1000\n",
      "112247/112247 [==============================] - 3s - loss: 481.0598 - rmse_: 21.8804 - val_loss: 441.6272 - val_rmse_: 20.8476\n",
      "Epoch 26/1000\n",
      "112247/112247 [==============================] - 3s - loss: 480.0361 - rmse_: 21.8599 - val_loss: 440.8190 - val_rmse_: 20.8259\n",
      "Epoch 27/1000\n",
      "112247/112247 [==============================] - 3s - loss: 479.3398 - rmse_: 21.8539 - val_loss: 440.2855 - val_rmse_: 20.8133\n",
      "Epoch 28/1000\n",
      "112247/112247 [==============================] - 3s - loss: 478.7233 - rmse_: 21.8310 - val_loss: 440.4194 - val_rmse_: 20.8236\n",
      "Epoch 29/1000\n",
      "112247/112247 [==============================] - 3s - loss: 478.5678 - rmse_: 21.8228 - val_loss: 439.2404 - val_rmse_: 20.7887\n",
      "Epoch 30/1000\n",
      "112247/112247 [==============================] - 3s - loss: 477.1843 - rmse_: 21.7941 - val_loss: 437.8383 - val_rmse_: 20.7452\n",
      "Epoch 31/1000\n",
      "112247/112247 [==============================] - 3s - loss: 476.8849 - rmse_: 21.7912 - val_loss: 438.2899 - val_rmse_: 20.7693\n",
      "Epoch 32/1000\n",
      "112247/112247 [==============================] - 3s - loss: 476.2487 - rmse_: 21.7807 - val_loss: 438.3689 - val_rmse_: 20.7790\n",
      "Epoch 33/1000\n",
      "112247/112247 [==============================] - 3s - loss: 475.3101 - rmse_: 21.7508 - val_loss: 436.4984 - val_rmse_: 20.7205\n",
      "Epoch 34/1000\n",
      "112247/112247 [==============================] - 3s - loss: 474.4453 - rmse_: 21.7188 - val_loss: 435.4135 - val_rmse_: 20.6874\n",
      "Epoch 35/1000\n",
      "112247/112247 [==============================] - 3s - loss: 474.3303 - rmse_: 21.7345 - val_loss: 435.1534 - val_rmse_: 20.6877\n",
      "Epoch 36/1000\n",
      "112247/112247 [==============================] - 3s - loss: 473.2065 - rmse_: 21.7036 - val_loss: 435.6381 - val_rmse_: 20.7125\n",
      "Epoch 37/1000\n",
      "112247/112247 [==============================] - 3s - loss: 472.5902 - rmse_: 21.6909 - val_loss: 436.1851 - val_rmse_: 20.7373\n",
      "Epoch 38/1000\n",
      "112247/112247 [==============================] - 3s - loss: 471.9005 - rmse_: 21.6770 - val_loss: 434.5615 - val_rmse_: 20.6913\n",
      "Epoch 39/1000\n",
      "112247/112247 [==============================] - 3s - loss: 470.3745 - rmse_: 21.6416 - val_loss: 434.7476 - val_rmse_: 20.7040\n",
      "Epoch 40/1000\n",
      "112247/112247 [==============================] - 3s - loss: 470.0663 - rmse_: 21.6326 - val_loss: 432.8705 - val_rmse_: 20.6495\n",
      "Epoch 41/1000\n",
      "112247/112247 [==============================] - 3s - loss: 469.5763 - rmse_: 21.6217 - val_loss: 433.0948 - val_rmse_: 20.6642\n",
      "Epoch 42/1000\n",
      "112247/112247 [==============================] - 3s - loss: 468.0172 - rmse_: 21.5938 - val_loss: 432.3702 - val_rmse_: 20.6483\n",
      "Epoch 43/1000\n",
      "112247/112247 [==============================] - 3s - loss: 467.7404 - rmse_: 21.5782 - val_loss: 430.8839 - val_rmse_: 20.6057\n",
      "Epoch 44/1000\n",
      "112247/112247 [==============================] - 3s - loss: 465.8750 - rmse_: 21.5380 - val_loss: 430.3244 - val_rmse_: 20.5954\n",
      "Epoch 45/1000\n",
      "112247/112247 [==============================] - 3s - loss: 465.7201 - rmse_: 21.5267 - val_loss: 430.4348 - val_rmse_: 20.6067\n",
      "Epoch 46/1000\n",
      "112247/112247 [==============================] - 3s - loss: 464.6053 - rmse_: 21.5082 - val_loss: 429.1541 - val_rmse_: 20.5726\n",
      "Epoch 47/1000\n",
      "112247/112247 [==============================] - 3s - loss: 464.0756 - rmse_: 21.5024 - val_loss: 427.9065 - val_rmse_: 20.5392\n",
      "Epoch 48/1000\n",
      "112247/112247 [==============================] - 3s - loss: 462.3602 - rmse_: 21.4603 - val_loss: 427.3523 - val_rmse_: 20.5282\n",
      "Epoch 49/1000\n",
      "112247/112247 [==============================] - 3s - loss: 461.9606 - rmse_: 21.4479 - val_loss: 426.3115 - val_rmse_: 20.5020\n",
      "Epoch 50/1000\n",
      "112247/112247 [==============================] - 3s - loss: 461.1423 - rmse_: 21.4211 - val_loss: 425.6011 - val_rmse_: 20.4854\n",
      "Epoch 51/1000\n",
      "112247/112247 [==============================] - 3s - loss: 460.1744 - rmse_: 21.4033 - val_loss: 424.0140 - val_rmse_: 20.4383\n",
      "Epoch 52/1000\n",
      "112247/112247 [==============================] - 3s - loss: 459.3518 - rmse_: 21.3880 - val_loss: 425.5987 - val_rmse_: 20.5008\n",
      "Epoch 53/1000\n",
      "112247/112247 [==============================] - 3s - loss: 459.5300 - rmse_: 21.3925 - val_loss: 424.6080 - val_rmse_: 20.4755\n",
      "Epoch 54/1000\n",
      "112247/112247 [==============================] - 3s - loss: 458.2560 - rmse_: 21.3600 - val_loss: 423.4986 - val_rmse_: 20.4465\n",
      "Epoch 55/1000\n",
      "112247/112247 [==============================] - 3s - loss: 457.5961 - rmse_: 21.3450 - val_loss: 422.7158 - val_rmse_: 20.4276\n",
      "Epoch 56/1000\n",
      "112247/112247 [==============================] - 3s - loss: 456.7137 - rmse_: 21.3253 - val_loss: 421.0992 - val_rmse_: 20.3810\n",
      "Epoch 57/1000\n",
      "112247/112247 [==============================] - 3s - loss: 455.6591 - rmse_: 21.3038 - val_loss: 421.4382 - val_rmse_: 20.4005\n",
      "Epoch 58/1000\n",
      "112247/112247 [==============================] - 3s - loss: 454.5220 - rmse_: 21.2773 - val_loss: 421.3355 - val_rmse_: 20.4044\n",
      "Epoch 59/1000\n",
      "112247/112247 [==============================] - 3s - loss: 454.5451 - rmse_: 21.2734 - val_loss: 420.3624 - val_rmse_: 20.3789\n",
      "Epoch 60/1000\n",
      "112247/112247 [==============================] - 3s - loss: 453.5097 - rmse_: 21.2412 - val_loss: 418.5715 - val_rmse_: 20.3239\n",
      "Epoch 61/1000\n",
      "112247/112247 [==============================] - 3s - loss: 452.4256 - rmse_: 21.2284 - val_loss: 418.4118 - val_rmse_: 20.3277\n",
      "Epoch 62/1000\n",
      "112247/112247 [==============================] - 3s - loss: 452.3429 - rmse_: 21.2240 - val_loss: 417.1410 - val_rmse_: 20.2904\n",
      "Epoch 63/1000\n",
      "112247/112247 [==============================] - 3s - loss: 452.0009 - rmse_: 21.2135 - val_loss: 416.8774 - val_rmse_: 20.2897\n",
      "Epoch 64/1000\n",
      "112247/112247 [==============================] - 3s - loss: 450.2526 - rmse_: 21.1810 - val_loss: 415.3539 - val_rmse_: 20.2441\n",
      "Epoch 65/1000\n",
      "112247/112247 [==============================] - 3s - loss: 450.4359 - rmse_: 21.1756 - val_loss: 417.4581 - val_rmse_: 20.3203\n",
      "Epoch 66/1000\n",
      "112247/112247 [==============================] - 3s - loss: 449.7348 - rmse_: 21.1527 - val_loss: 415.6442 - val_rmse_: 20.2679\n",
      "Epoch 67/1000\n",
      "112247/112247 [==============================] - 3s - loss: 448.9152 - rmse_: 21.1458 - val_loss: 414.7120 - val_rmse_: 20.2433\n",
      "Epoch 68/1000\n",
      "112247/112247 [==============================] - 3s - loss: 447.6903 - rmse_: 21.1117 - val_loss: 416.1918 - val_rmse_: 20.2953\n",
      "Epoch 69/1000\n",
      "112247/112247 [==============================] - 3s - loss: 448.0081 - rmse_: 21.1211 - val_loss: 413.8783 - val_rmse_: 20.2280\n",
      "Epoch 70/1000\n",
      "112247/112247 [==============================] - 3s - loss: 447.3582 - rmse_: 21.1020 - val_loss: 411.3391 - val_rmse_: 20.1461\n",
      "Epoch 71/1000\n",
      "112247/112247 [==============================] - 3s - loss: 445.0655 - rmse_: 21.0499 - val_loss: 413.3580 - val_rmse_: 20.2215\n",
      "Epoch 72/1000\n",
      "112247/112247 [==============================] - 3s - loss: 445.3119 - rmse_: 21.0567 - val_loss: 412.1016 - val_rmse_: 20.1871\n",
      "Epoch 73/1000\n",
      "112247/112247 [==============================] - 3s - loss: 445.2377 - rmse_: 21.0450 - val_loss: 412.9771 - val_rmse_: 20.2186\n",
      "Epoch 74/1000\n",
      "112247/112247 [==============================] - 3s - loss: 444.6649 - rmse_: 21.0414 - val_loss: 410.8894 - val_rmse_: 20.1578\n",
      "Epoch 75/1000\n",
      "112247/112247 [==============================] - 3s - loss: 444.0088 - rmse_: 21.0273 - val_loss: 411.7009 - val_rmse_: 20.1890\n",
      "Epoch 76/1000\n",
      "112247/112247 [==============================] - 3s - loss: 442.4116 - rmse_: 20.9925 - val_loss: 409.5004 - val_rmse_: 20.1232\n",
      "Epoch 77/1000\n",
      "112247/112247 [==============================] - 3s - loss: 442.6661 - rmse_: 20.9900 - val_loss: 409.8871 - val_rmse_: 20.1410\n",
      "Epoch 78/1000\n",
      "112247/112247 [==============================] - 3s - loss: 441.0069 - rmse_: 20.9534 - val_loss: 408.3928 - val_rmse_: 20.0962\n",
      "Epoch 79/1000\n",
      "112247/112247 [==============================] - 3s - loss: 441.5473 - rmse_: 20.9681 - val_loss: 407.5297 - val_rmse_: 20.0743\n",
      "Epoch 80/1000\n",
      "112247/112247 [==============================] - 3s - loss: 440.6682 - rmse_: 20.9442 - val_loss: 407.7834 - val_rmse_: 20.0877\n",
      "Epoch 81/1000\n",
      "112247/112247 [==============================] - 3s - loss: 440.9859 - rmse_: 20.9548 - val_loss: 406.8512 - val_rmse_: 20.0611\n",
      "Epoch 82/1000\n",
      "112247/112247 [==============================] - 3s - loss: 440.5305 - rmse_: 20.9424 - val_loss: 406.2162 - val_rmse_: 20.0444\n",
      "Epoch 83/1000\n",
      "112247/112247 [==============================] - 3s - loss: 439.5003 - rmse_: 20.9143 - val_loss: 406.0560 - val_rmse_: 20.0446\n",
      "Epoch 84/1000\n",
      "112247/112247 [==============================] - 3s - loss: 438.3648 - rmse_: 20.8909 - val_loss: 404.7299 - val_rmse_: 20.0034\n",
      "Epoch 85/1000\n",
      "112247/112247 [==============================] - 3s - loss: 438.6084 - rmse_: 20.8942 - val_loss: 404.0545 - val_rmse_: 19.9878\n",
      "Epoch 86/1000\n",
      "112247/112247 [==============================] - 3s - loss: 436.7354 - rmse_: 20.8483 - val_loss: 405.4608 - val_rmse_: 20.0392\n",
      "Epoch 87/1000\n",
      "112247/112247 [==============================] - 3s - loss: 436.5340 - rmse_: 20.8457 - val_loss: 405.4420 - val_rmse_: 20.0430\n",
      "Epoch 88/1000\n",
      "112247/112247 [==============================] - 3s - loss: 436.5987 - rmse_: 20.8584 - val_loss: 401.7395 - val_rmse_: 19.9182\n",
      "Epoch 89/1000\n",
      "112247/112247 [==============================] - 3s - loss: 434.8933 - rmse_: 20.8156 - val_loss: 403.0606 - val_rmse_: 19.9760\n",
      "Epoch 90/1000\n",
      "112247/112247 [==============================] - 3s - loss: 435.1564 - rmse_: 20.8070 - val_loss: 402.0278 - val_rmse_: 19.9455\n",
      "Epoch 91/1000\n",
      "112247/112247 [==============================] - 3s - loss: 434.5387 - rmse_: 20.8011 - val_loss: 402.6145 - val_rmse_: 19.9685\n",
      "Epoch 92/1000\n",
      "112247/112247 [==============================] - 3s - loss: 435.4407 - rmse_: 20.8200 - val_loss: 400.7120 - val_rmse_: 19.9094\n",
      "Epoch 93/1000\n",
      "112247/112247 [==============================] - 3s - loss: 433.5415 - rmse_: 20.7772 - val_loss: 401.5864 - val_rmse_: 19.9433\n",
      "Epoch 94/1000\n",
      "112247/112247 [==============================] - 3s - loss: 433.8274 - rmse_: 20.7883 - val_loss: 399.6439 - val_rmse_: 19.8831\n",
      "Epoch 95/1000\n",
      "112247/112247 [==============================] - 3s - loss: 432.5363 - rmse_: 20.7565 - val_loss: 400.8793 - val_rmse_: 19.9282\n",
      "Epoch 96/1000\n",
      "112247/112247 [==============================] - 3s - loss: 432.5303 - rmse_: 20.7496 - val_loss: 400.3451 - val_rmse_: 19.9155\n",
      "Epoch 97/1000\n",
      "112247/112247 [==============================] - 3s - loss: 431.8874 - rmse_: 20.7341 - val_loss: 398.0059 - val_rmse_: 19.8429\n",
      "Epoch 98/1000\n",
      "112247/112247 [==============================] - 3s - loss: 430.4444 - rmse_: 20.7017 - val_loss: 398.9935 - val_rmse_: 19.8803\n",
      "Epoch 99/1000\n",
      "112247/112247 [==============================] - 3s - loss: 431.1899 - rmse_: 20.7165 - val_loss: 398.2174 - val_rmse_: 19.8586\n",
      "Epoch 100/1000\n",
      "112247/112247 [==============================] - 3s - loss: 430.9934 - rmse_: 20.7194 - val_loss: 397.6645 - val_rmse_: 19.8450\n",
      "Epoch 101/1000\n",
      "112247/112247 [==============================] - 3s - loss: 430.0801 - rmse_: 20.6902 - val_loss: 397.3090 - val_rmse_: 19.8369\n",
      "Epoch 102/1000\n",
      "112247/112247 [==============================] - 3s - loss: 428.5562 - rmse_: 20.6620 - val_loss: 397.1055 - val_rmse_: 19.8351\n",
      "Epoch 103/1000\n",
      "112247/112247 [==============================] - 3s - loss: 428.9387 - rmse_: 20.6668 - val_loss: 396.2253 - val_rmse_: 19.8098\n",
      "Epoch 104/1000\n",
      "112247/112247 [==============================] - 3s - loss: 428.4738 - rmse_: 20.6531 - val_loss: 394.1731 - val_rmse_: 19.7396\n",
      "Epoch 105/1000\n",
      "112247/112247 [==============================] - 3s - loss: 427.6522 - rmse_: 20.6388 - val_loss: 394.2477 - val_rmse_: 19.7495\n",
      "Epoch 106/1000\n",
      "112247/112247 [==============================] - 2s - loss: 427.8727 - rmse_: 20.6400 - val_loss: 394.6697 - val_rmse_: 19.7685\n",
      "Epoch 107/1000\n",
      "112247/112247 [==============================] - 3s - loss: 427.2925 - rmse_: 20.6277 - val_loss: 394.7434 - val_rmse_: 19.7774\n",
      "Epoch 108/1000\n",
      "112247/112247 [==============================] - 3s - loss: 428.4246 - rmse_: 20.6596 - val_loss: 392.8111 - val_rmse_: 19.7145\n",
      "Epoch 109/1000\n",
      "112247/112247 [==============================] - 3s - loss: 426.8604 - rmse_: 20.6164 - val_loss: 393.8435 - val_rmse_: 19.7562\n",
      "Epoch 110/1000\n",
      "112247/112247 [==============================] - 3s - loss: 427.2865 - rmse_: 20.6245 - val_loss: 394.9010 - val_rmse_: 19.7921\n",
      "Epoch 111/1000\n",
      "112247/112247 [==============================] - 3s - loss: 426.8828 - rmse_: 20.6148 - val_loss: 394.2022 - val_rmse_: 19.7722\n",
      "Epoch 112/1000\n",
      "112247/112247 [==============================] - 3s - loss: 426.3495 - rmse_: 20.6034 - val_loss: 391.7019 - val_rmse_: 19.6944\n",
      "Epoch 113/1000\n",
      "112247/112247 [==============================] - 3s - loss: 424.7896 - rmse_: 20.5683 - val_loss: 391.1689 - val_rmse_: 19.6797\n",
      "Epoch 114/1000\n",
      "112247/112247 [==============================] - 3s - loss: 424.4819 - rmse_: 20.5510 - val_loss: 391.8541 - val_rmse_: 19.7079\n",
      "Epoch 115/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112247/112247 [==============================] - 3s - loss: 423.1475 - rmse_: 20.5265 - val_loss: 389.5608 - val_rmse_: 19.6320\n",
      "Epoch 116/1000\n",
      "112247/112247 [==============================] - 3s - loss: 423.7812 - rmse_: 20.5461 - val_loss: 389.9407 - val_rmse_: 19.6507\n",
      "Epoch 117/1000\n",
      "112247/112247 [==============================] - 3s - loss: 424.1762 - rmse_: 20.5455 - val_loss: 388.6254 - val_rmse_: 19.5996\n",
      "Epoch 118/1000\n",
      "112247/112247 [==============================] - 3s - loss: 424.5146 - rmse_: 20.5614 - val_loss: 389.5355 - val_rmse_: 19.6455\n",
      "Epoch 119/1000\n",
      "112247/112247 [==============================] - 3s - loss: 423.1674 - rmse_: 20.5309 - val_loss: 388.6032 - val_rmse_: 19.6167\n",
      "Epoch 120/1000\n",
      "112247/112247 [==============================] - 3s - loss: 421.4407 - rmse_: 20.4849 - val_loss: 388.6604 - val_rmse_: 19.6247\n",
      "Epoch 121/1000\n",
      "112247/112247 [==============================] - 3s - loss: 422.0492 - rmse_: 20.4969 - val_loss: 388.0381 - val_rmse_: 19.6021\n",
      "Epoch 122/1000\n",
      "112247/112247 [==============================] - 3s - loss: 421.5780 - rmse_: 20.4891 - val_loss: 387.6719 - val_rmse_: 19.5971\n",
      "Epoch 123/1000\n",
      "112247/112247 [==============================] - 3s - loss: 420.6121 - rmse_: 20.4559 - val_loss: 387.4128 - val_rmse_: 19.5908\n",
      "Epoch 124/1000\n",
      "112247/112247 [==============================] - 3s - loss: 420.4831 - rmse_: 20.4700 - val_loss: 386.4464 - val_rmse_: 19.5614\n",
      "Epoch 125/1000\n",
      "112247/112247 [==============================] - 3s - loss: 421.1837 - rmse_: 20.4765 - val_loss: 386.0373 - val_rmse_: 19.5489\n",
      "Epoch 126/1000\n",
      "112247/112247 [==============================] - 3s - loss: 420.2004 - rmse_: 20.4496 - val_loss: 386.2116 - val_rmse_: 19.5584\n",
      "Epoch 127/1000\n",
      "112247/112247 [==============================] - 3s - loss: 420.1883 - rmse_: 20.4548 - val_loss: 386.0831 - val_rmse_: 19.5600\n",
      "Epoch 128/1000\n",
      "112247/112247 [==============================] - 3s - loss: 420.2140 - rmse_: 20.4519 - val_loss: 386.2599 - val_rmse_: 19.5715\n",
      "Epoch 129/1000\n",
      "112247/112247 [==============================] - 3s - loss: 419.1313 - rmse_: 20.4308 - val_loss: 385.2155 - val_rmse_: 19.5386\n",
      "Epoch 130/1000\n",
      "112247/112247 [==============================] - 3s - loss: 418.1093 - rmse_: 20.4096 - val_loss: 383.9529 - val_rmse_: 19.4970\n",
      "Epoch 131/1000\n",
      "112247/112247 [==============================] - 3s - loss: 417.4338 - rmse_: 20.3846 - val_loss: 384.6425 - val_rmse_: 19.5274\n",
      "Epoch 132/1000\n",
      "112247/112247 [==============================] - 3s - loss: 418.8693 - rmse_: 20.4272 - val_loss: 383.5203 - val_rmse_: 19.4822\n",
      "Epoch 133/1000\n",
      "112247/112247 [==============================] - 3s - loss: 417.1398 - rmse_: 20.3766 - val_loss: 382.7508 - val_rmse_: 19.4658\n",
      "Epoch 134/1000\n",
      "112247/112247 [==============================] - 3s - loss: 415.5039 - rmse_: 20.3409 - val_loss: 382.4787 - val_rmse_: 19.4608\n",
      "Epoch 135/1000\n",
      "112247/112247 [==============================] - 3s - loss: 417.6362 - rmse_: 20.3920 - val_loss: 382.8692 - val_rmse_: 19.4809\n",
      "Epoch 136/1000\n",
      "112247/112247 [==============================] - 3s - loss: 415.1375 - rmse_: 20.3306 - val_loss: 381.5918 - val_rmse_: 19.4323\n",
      "Epoch 137/1000\n",
      "112247/112247 [==============================] - 3s - loss: 414.2944 - rmse_: 20.3087 - val_loss: 382.1715 - val_rmse_: 19.4652\n",
      "Epoch 138/1000\n",
      "112247/112247 [==============================] - 3s - loss: 415.4530 - rmse_: 20.3344 - val_loss: 382.1858 - val_rmse_: 19.4689\n",
      "Epoch 139/1000\n",
      "112247/112247 [==============================] - 3s - loss: 414.3577 - rmse_: 20.3203 - val_loss: 381.4569 - val_rmse_: 19.4484\n",
      "Epoch 140/1000\n",
      "112247/112247 [==============================] - 3s - loss: 413.7744 - rmse_: 20.3024 - val_loss: 380.8287 - val_rmse_: 19.4291\n",
      "Epoch 141/1000\n",
      "112247/112247 [==============================] - 3s - loss: 414.0922 - rmse_: 20.3102 - val_loss: 380.2313 - val_rmse_: 19.4103\n",
      "Epoch 142/1000\n",
      "112247/112247 [==============================] - 3s - loss: 413.0305 - rmse_: 20.2814 - val_loss: 380.2901 - val_rmse_: 19.4167\n",
      "Epoch 143/1000\n",
      "112247/112247 [==============================] - 3s - loss: 413.1676 - rmse_: 20.2871 - val_loss: 379.1126 - val_rmse_: 19.3735\n",
      "Epoch 144/1000\n",
      "112247/112247 [==============================] - 3s - loss: 413.0330 - rmse_: 20.2800 - val_loss: 380.0557 - val_rmse_: 19.4157\n",
      "Epoch 145/1000\n",
      "112247/112247 [==============================] - 2s - loss: 412.3447 - rmse_: 20.2635 - val_loss: 379.6713 - val_rmse_: 19.4028\n",
      "Epoch 146/1000\n",
      "112247/112247 [==============================] - 3s - loss: 412.4242 - rmse_: 20.2644 - val_loss: 379.3018 - val_rmse_: 19.3934\n",
      "Epoch 147/1000\n",
      "112247/112247 [==============================] - 3s - loss: 411.0991 - rmse_: 20.2309 - val_loss: 378.1693 - val_rmse_: 19.3571\n",
      "Epoch 148/1000\n",
      "112247/112247 [==============================] - 2s - loss: 411.2737 - rmse_: 20.2300 - val_loss: 378.8505 - val_rmse_: 19.3868\n",
      "Epoch 149/1000\n",
      "112247/112247 [==============================] - 3s - loss: 411.9412 - rmse_: 20.2495 - val_loss: 377.8431 - val_rmse_: 19.3529\n",
      "Epoch 150/1000\n",
      "112247/112247 [==============================] - 3s - loss: 410.5144 - rmse_: 20.2169 - val_loss: 377.4190 - val_rmse_: 19.3442\n",
      "Epoch 151/1000\n",
      "112247/112247 [==============================] - 3s - loss: 410.9469 - rmse_: 20.2268 - val_loss: 377.0807 - val_rmse_: 19.3302\n",
      "Epoch 152/1000\n",
      "112247/112247 [==============================] - 3s - loss: 409.3990 - rmse_: 20.1955 - val_loss: 376.2384 - val_rmse_: 19.3040\n",
      "Epoch 153/1000\n",
      "112247/112247 [==============================] - 3s - loss: 406.8266 - rmse_: 20.1333 - val_loss: 376.7561 - val_rmse_: 19.3312\n",
      "Epoch 154/1000\n",
      "112247/112247 [==============================] - 3s - loss: 408.6991 - rmse_: 20.1664 - val_loss: 375.6848 - val_rmse_: 19.2967\n",
      "Epoch 155/1000\n",
      "112247/112247 [==============================] - 3s - loss: 408.4198 - rmse_: 20.1643 - val_loss: 374.7601 - val_rmse_: 19.2626\n",
      "Epoch 156/1000\n",
      "112247/112247 [==============================] - 3s - loss: 408.7797 - rmse_: 20.1722 - val_loss: 375.1205 - val_rmse_: 19.2785\n",
      "Epoch 157/1000\n",
      "112247/112247 [==============================] - 3s - loss: 408.1612 - rmse_: 20.1630 - val_loss: 375.0287 - val_rmse_: 19.2809\n",
      "Epoch 158/1000\n",
      "112247/112247 [==============================] - 3s - loss: 405.8311 - rmse_: 20.1080 - val_loss: 374.7350 - val_rmse_: 19.2775\n",
      "Epoch 159/1000\n",
      "112247/112247 [==============================] - 3s - loss: 406.8810 - rmse_: 20.1372 - val_loss: 374.1957 - val_rmse_: 19.2584\n",
      "Epoch 160/1000\n",
      "112247/112247 [==============================] - 3s - loss: 406.6531 - rmse_: 20.1253 - val_loss: 374.5206 - val_rmse_: 19.2721\n",
      "Epoch 161/1000\n",
      "112247/112247 [==============================] - 3s - loss: 408.2666 - rmse_: 20.1628 - val_loss: 374.7681 - val_rmse_: 19.2840\n",
      "Epoch 162/1000\n",
      "112247/112247 [==============================] - 3s - loss: 407.3119 - rmse_: 20.1422 - val_loss: 373.3692 - val_rmse_: 19.2352\n",
      "Epoch 163/1000\n",
      "112247/112247 [==============================] - 3s - loss: 404.5470 - rmse_: 20.0670 - val_loss: 374.2053 - val_rmse_: 19.2718\n",
      "Epoch 164/1000\n",
      "112247/112247 [==============================] - 3s - loss: 405.6417 - rmse_: 20.0992 - val_loss: 372.5419 - val_rmse_: 19.2157\n",
      "Epoch 165/1000\n",
      "112247/112247 [==============================] - 3s - loss: 404.9246 - rmse_: 20.0806 - val_loss: 372.3387 - val_rmse_: 19.2085\n",
      "Epoch 166/1000\n",
      "112247/112247 [==============================] - 3s - loss: 406.2565 - rmse_: 20.1152 - val_loss: 371.9895 - val_rmse_: 19.2062\n",
      "Epoch 167/1000\n",
      "112247/112247 [==============================] - 3s - loss: 405.5715 - rmse_: 20.0954 - val_loss: 371.4143 - val_rmse_: 19.1845\n",
      "Epoch 168/1000\n",
      "112247/112247 [==============================] - 3s - loss: 404.9822 - rmse_: 20.0871 - val_loss: 371.3441 - val_rmse_: 19.1890\n",
      "Epoch 169/1000\n",
      "112247/112247 [==============================] - 3s - loss: 403.6616 - rmse_: 20.0448 - val_loss: 370.6849 - val_rmse_: 19.1644\n",
      "Epoch 170/1000\n",
      "112247/112247 [==============================] - 3s - loss: 403.4174 - rmse_: 20.0364 - val_loss: 371.0998 - val_rmse_: 19.1864\n",
      "Epoch 171/1000\n",
      "112247/112247 [==============================] - 3s - loss: 402.5606 - rmse_: 20.0174 - val_loss: 370.9790 - val_rmse_: 19.1860\n",
      "Epoch 172/1000\n",
      "112247/112247 [==============================] - 3s - loss: 403.3026 - rmse_: 20.0423 - val_loss: 370.0292 - val_rmse_: 19.1502\n",
      "Epoch 173/1000\n",
      "112247/112247 [==============================] - 3s - loss: 404.1346 - rmse_: 20.0650 - val_loss: 369.8721 - val_rmse_: 19.1508\n",
      "Epoch 174/1000\n",
      "112247/112247 [==============================] - 3s - loss: 402.1584 - rmse_: 20.0094 - val_loss: 369.5801 - val_rmse_: 19.1433\n",
      "Epoch 175/1000\n",
      "112247/112247 [==============================] - 3s - loss: 402.7060 - rmse_: 20.0325 - val_loss: 369.5288 - val_rmse_: 19.1444\n",
      "Epoch 176/1000\n",
      "112247/112247 [==============================] - 3s - loss: 402.0419 - rmse_: 20.0054 - val_loss: 368.7866 - val_rmse_: 19.1130\n",
      "Epoch 177/1000\n",
      "112247/112247 [==============================] - 3s - loss: 401.3724 - rmse_: 19.9957 - val_loss: 369.0548 - val_rmse_: 19.1332\n",
      "Epoch 178/1000\n",
      "112247/112247 [==============================] - 3s - loss: 403.9531 - rmse_: 20.0508 - val_loss: 368.5174 - val_rmse_: 19.1137\n",
      "Epoch 179/1000\n",
      "112247/112247 [==============================] - 3s - loss: 400.7573 - rmse_: 19.9825 - val_loss: 368.1783 - val_rmse_: 19.1095\n",
      "Epoch 180/1000\n",
      "112247/112247 [==============================] - 3s - loss: 399.4917 - rmse_: 19.9442 - val_loss: 367.9202 - val_rmse_: 19.1013\n",
      "Epoch 181/1000\n",
      "112247/112247 [==============================] - 3s - loss: 400.5686 - rmse_: 19.9729 - val_loss: 367.5810 - val_rmse_: 19.0945\n",
      "Epoch 182/1000\n",
      "112247/112247 [==============================] - 3s - loss: 398.1702 - rmse_: 19.9095 - val_loss: 367.4457 - val_rmse_: 19.0903\n",
      "Epoch 183/1000\n",
      "112247/112247 [==============================] - 3s - loss: 398.6387 - rmse_: 19.9239 - val_loss: 366.7111 - val_rmse_: 19.0629\n",
      "Epoch 184/1000\n",
      "112247/112247 [==============================] - 3s - loss: 401.1692 - rmse_: 19.9903 - val_loss: 366.8098 - val_rmse_: 19.0748\n",
      "Epoch 185/1000\n",
      "112247/112247 [==============================] - 3s - loss: 399.1373 - rmse_: 19.9324 - val_loss: 366.6130 - val_rmse_: 19.0679\n",
      "Epoch 186/1000\n",
      "112247/112247 [==============================] - 3s - loss: 397.2172 - rmse_: 19.8869 - val_loss: 366.0704 - val_rmse_: 19.0560\n",
      "Epoch 187/1000\n",
      "112247/112247 [==============================] - 3s - loss: 399.3074 - rmse_: 19.9370 - val_loss: 365.8046 - val_rmse_: 19.0501\n",
      "Epoch 188/1000\n",
      "112247/112247 [==============================] - 3s - loss: 398.3221 - rmse_: 19.9079 - val_loss: 365.2942 - val_rmse_: 19.0269\n",
      "Epoch 189/1000\n",
      "112247/112247 [==============================] - 3s - loss: 396.6767 - rmse_: 19.8760 - val_loss: 365.2717 - val_rmse_: 19.0350\n",
      "Epoch 190/1000\n",
      "112247/112247 [==============================] - 3s - loss: 399.1433 - rmse_: 19.9337 - val_loss: 365.3889 - val_rmse_: 19.0372\n",
      "Epoch 191/1000\n",
      "112247/112247 [==============================] - 3s - loss: 397.2176 - rmse_: 19.8886 - val_loss: 364.5512 - val_rmse_: 19.0151\n",
      "Epoch 192/1000\n",
      "112247/112247 [==============================] - 3s - loss: 397.5251 - rmse_: 19.8873 - val_loss: 364.7269 - val_rmse_: 19.0218\n",
      "Epoch 193/1000\n",
      "112247/112247 [==============================] - 3s - loss: 396.7704 - rmse_: 19.8757 - val_loss: 364.3241 - val_rmse_: 19.0114\n",
      "Epoch 194/1000\n",
      "112247/112247 [==============================] - 3s - loss: 393.9490 - rmse_: 19.8029 - val_loss: 363.7247 - val_rmse_: 18.9945\n",
      "Epoch 195/1000\n",
      "112247/112247 [==============================] - 3s - loss: 396.9219 - rmse_: 19.8822 - val_loss: 363.4016 - val_rmse_: 18.9840\n",
      "Epoch 196/1000\n",
      "112247/112247 [==============================] - 3s - loss: 394.7057 - rmse_: 19.8265 - val_loss: 363.9354 - val_rmse_: 19.0083\n",
      "Epoch 197/1000\n",
      "112247/112247 [==============================] - 3s - loss: 396.4203 - rmse_: 19.8616 - val_loss: 363.2856 - val_rmse_: 18.9861\n",
      "Epoch 198/1000\n",
      "112247/112247 [==============================] - 3s - loss: 396.3288 - rmse_: 19.8647 - val_loss: 363.1935 - val_rmse_: 18.9832\n",
      "Epoch 199/1000\n",
      "112247/112247 [==============================] - 3s - loss: 395.7585 - rmse_: 19.8502 - val_loss: 362.6445 - val_rmse_: 18.9655\n",
      "Epoch 200/1000\n",
      "112247/112247 [==============================] - 3s - loss: 395.2802 - rmse_: 19.8432 - val_loss: 362.6505 - val_rmse_: 18.9626\n",
      "Epoch 201/1000\n",
      "112247/112247 [==============================] - 3s - loss: 394.6589 - rmse_: 19.8258 - val_loss: 362.0249 - val_rmse_: 18.9517\n",
      "Epoch 202/1000\n",
      "112247/112247 [==============================] - 3s - loss: 394.0554 - rmse_: 19.8054 - val_loss: 361.7712 - val_rmse_: 18.9470\n",
      "Epoch 203/1000\n",
      "112247/112247 [==============================] - 2s - loss: 395.4650 - rmse_: 19.8427 - val_loss: 362.0243 - val_rmse_: 18.9582\n",
      "Epoch 204/1000\n",
      "112247/112247 [==============================] - 3s - loss: 393.0708 - rmse_: 19.7843 - val_loss: 361.0209 - val_rmse_: 18.9121\n",
      "Epoch 205/1000\n",
      "112247/112247 [==============================] - 3s - loss: 393.8381 - rmse_: 19.8049 - val_loss: 361.2658 - val_rmse_: 18.9311\n",
      "Epoch 206/1000\n",
      "112247/112247 [==============================] - 3s - loss: 393.8903 - rmse_: 19.8105 - val_loss: 360.8620 - val_rmse_: 18.9237\n",
      "Epoch 207/1000\n",
      "112247/112247 [==============================] - 3s - loss: 393.4622 - rmse_: 19.7933 - val_loss: 360.3625 - val_rmse_: 18.9086\n",
      "Epoch 208/1000\n",
      "112247/112247 [==============================] - 3s - loss: 392.3085 - rmse_: 19.7626 - val_loss: 360.1877 - val_rmse_: 18.9002\n",
      "Epoch 209/1000\n",
      "112247/112247 [==============================] - 3s - loss: 393.3992 - rmse_: 19.7960 - val_loss: 359.9313 - val_rmse_: 18.8926\n",
      "Epoch 210/1000\n",
      "112247/112247 [==============================] - 3s - loss: 391.3043 - rmse_: 19.7431 - val_loss: 359.8433 - val_rmse_: 18.8780\n",
      "Epoch 211/1000\n",
      "112247/112247 [==============================] - 3s - loss: 390.9334 - rmse_: 19.7279 - val_loss: 358.9096 - val_rmse_: 18.8544\n",
      "Epoch 212/1000\n",
      "112247/112247 [==============================] - 3s - loss: 392.3749 - rmse_: 19.7619 - val_loss: 359.3703 - val_rmse_: 18.8870\n",
      "Epoch 213/1000\n",
      "112247/112247 [==============================] - 3s - loss: 392.1935 - rmse_: 19.7567 - val_loss: 359.2258 - val_rmse_: 18.8829\n",
      "Epoch 214/1000\n",
      "112247/112247 [==============================] - 3s - loss: 391.6774 - rmse_: 19.7478 - val_loss: 358.8629 - val_rmse_: 18.8696\n",
      "Epoch 215/1000\n",
      "112247/112247 [==============================] - 3s - loss: 391.7624 - rmse_: 19.7513 - val_loss: 358.5790 - val_rmse_: 18.8611\n",
      "Epoch 216/1000\n",
      "112247/112247 [==============================] - 3s - loss: 390.8812 - rmse_: 19.7305 - val_loss: 358.2310 - val_rmse_: 18.8486\n",
      "Epoch 217/1000\n",
      "112247/112247 [==============================] - 3s - loss: 391.3822 - rmse_: 19.7385 - val_loss: 358.1288 - val_rmse_: 18.8529\n",
      "Epoch 218/1000\n",
      "112247/112247 [==============================] - 3s - loss: 389.2186 - rmse_: 19.6810 - val_loss: 357.7153 - val_rmse_: 18.8401\n",
      "Epoch 219/1000\n",
      "112247/112247 [==============================] - 3s - loss: 390.1046 - rmse_: 19.7113 - val_loss: 357.4042 - val_rmse_: 18.8290\n",
      "Epoch 220/1000\n",
      "112247/112247 [==============================] - 4s - loss: 390.7354 - rmse_: 19.7262 - val_loss: 357.1510 - val_rmse_: 18.8240\n",
      "Epoch 221/1000\n",
      "112247/112247 [==============================] - 2s - loss: 389.1818 - rmse_: 19.6910 - val_loss: 357.4109 - val_rmse_: 18.8340\n",
      "Epoch 222/1000\n",
      "112247/112247 [==============================] - 3s - loss: 390.5817 - rmse_: 19.7248 - val_loss: 356.8801 - val_rmse_: 18.8221\n",
      "Epoch 223/1000\n",
      "112247/112247 [==============================] - 3s - loss: 388.1588 - rmse_: 19.6597 - val_loss: 356.6095 - val_rmse_: 18.8112\n",
      "Epoch 224/1000\n",
      "112247/112247 [==============================] - 3s - loss: 389.4553 - rmse_: 19.6921 - val_loss: 356.7983 - val_rmse_: 18.8224\n",
      "Epoch 225/1000\n",
      "112247/112247 [==============================] - 3s - loss: 387.5271 - rmse_: 19.6442 - val_loss: 356.0456 - val_rmse_: 18.7976\n",
      "Epoch 226/1000\n",
      "112247/112247 [==============================] - 3s - loss: 388.9171 - rmse_: 19.6710 - val_loss: 355.7620 - val_rmse_: 18.7915\n",
      "Epoch 227/1000\n",
      "112247/112247 [==============================] - 3s - loss: 389.3357 - rmse_: 19.6905 - val_loss: 355.6012 - val_rmse_: 18.7828\n",
      "Epoch 228/1000\n",
      "112247/112247 [==============================] - 3s - loss: 387.5977 - rmse_: 19.6438 - val_loss: 355.4244 - val_rmse_: 18.7819\n",
      "Epoch 229/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112247/112247 [==============================] - 3s - loss: 386.5744 - rmse_: 19.6175 - val_loss: 355.2381 - val_rmse_: 18.7816\n",
      "Epoch 230/1000\n",
      "112247/112247 [==============================] - 3s - loss: 387.3037 - rmse_: 19.6307 - val_loss: 354.9211 - val_rmse_: 18.7688\n",
      "Epoch 231/1000\n",
      "112247/112247 [==============================] - 3s - loss: 386.3714 - rmse_: 19.6183 - val_loss: 354.7565 - val_rmse_: 18.7626\n",
      "Epoch 232/1000\n",
      "112247/112247 [==============================] - 3s - loss: 385.6592 - rmse_: 19.6013 - val_loss: 354.5439 - val_rmse_: 18.7559\n",
      "Epoch 233/1000\n",
      "112247/112247 [==============================] - 3s - loss: 385.6226 - rmse_: 19.5974 - val_loss: 354.5570 - val_rmse_: 18.7631\n",
      "Epoch 234/1000\n",
      "112247/112247 [==============================] - 3s - loss: 386.3547 - rmse_: 19.6111 - val_loss: 354.2143 - val_rmse_: 18.7503\n",
      "Epoch 235/1000\n",
      "112247/112247 [==============================] - 3s - loss: 386.4602 - rmse_: 19.6177 - val_loss: 354.2374 - val_rmse_: 18.7567\n",
      "Epoch 236/1000\n",
      "112247/112247 [==============================] - 3s - loss: 386.0774 - rmse_: 19.6035 - val_loss: 353.4354 - val_rmse_: 18.7277\n",
      "Epoch 237/1000\n",
      "112247/112247 [==============================] - 3s - loss: 384.4751 - rmse_: 19.5626 - val_loss: 353.2487 - val_rmse_: 18.7258\n",
      "Epoch 238/1000\n",
      "112247/112247 [==============================] - 3s - loss: 386.1757 - rmse_: 19.6059 - val_loss: 353.1409 - val_rmse_: 18.7118\n",
      "Epoch 239/1000\n",
      "112247/112247 [==============================] - 3s - loss: 385.5459 - rmse_: 19.5942 - val_loss: 353.2207 - val_rmse_: 18.7279\n",
      "Epoch 240/1000\n",
      "112247/112247 [==============================] - 3s - loss: 384.3859 - rmse_: 19.5657 - val_loss: 352.4823 - val_rmse_: 18.6965\n",
      "Epoch 241/1000\n",
      "112247/112247 [==============================] - 3s - loss: 381.5670 - rmse_: 19.4913 - val_loss: 352.5603 - val_rmse_: 18.7107\n",
      "Epoch 242/1000\n",
      "112247/112247 [==============================] - 3s - loss: 382.5653 - rmse_: 19.5161 - val_loss: 352.2758 - val_rmse_: 18.7043\n",
      "Epoch 243/1000\n",
      "112247/112247 [==============================] - 3s - loss: 383.4562 - rmse_: 19.5436 - val_loss: 352.0482 - val_rmse_: 18.6924\n",
      "Epoch 244/1000\n",
      "112247/112247 [==============================] - 3s - loss: 383.3927 - rmse_: 19.5393 - val_loss: 351.8355 - val_rmse_: 18.6830\n",
      "Epoch 245/1000\n",
      "112247/112247 [==============================] - 3s - loss: 383.7425 - rmse_: 19.5485 - val_loss: 351.5792 - val_rmse_: 18.6799\n",
      "Epoch 246/1000\n",
      "112247/112247 [==============================] - 3s - loss: 383.1209 - rmse_: 19.5344 - val_loss: 351.6144 - val_rmse_: 18.6821\n",
      "Epoch 247/1000\n",
      "112247/112247 [==============================] - 3s - loss: 383.6311 - rmse_: 19.5506 - val_loss: 351.0668 - val_rmse_: 18.6668\n",
      "Epoch 248/1000\n",
      "112247/112247 [==============================] - 3s - loss: 383.3420 - rmse_: 19.5346 - val_loss: 351.0817 - val_rmse_: 18.6733\n",
      "Epoch 249/1000\n",
      "112247/112247 [==============================] - 3s - loss: 380.9704 - rmse_: 19.4789 - val_loss: 350.8277 - val_rmse_: 18.6616\n",
      "Epoch 250/1000\n",
      "112247/112247 [==============================] - 3s - loss: 383.7096 - rmse_: 19.5468 - val_loss: 350.5997 - val_rmse_: 18.6586\n",
      "Epoch 251/1000\n",
      "112247/112247 [==============================] - 3s - loss: 382.9230 - rmse_: 19.5240 - val_loss: 350.4667 - val_rmse_: 18.6520\n",
      "Epoch 252/1000\n",
      "112247/112247 [==============================] - 3s - loss: 382.3884 - rmse_: 19.5159 - val_loss: 349.9178 - val_rmse_: 18.6324\n",
      "Epoch 253/1000\n",
      "112247/112247 [==============================] - 3s - loss: 382.2341 - rmse_: 19.5151 - val_loss: 349.9595 - val_rmse_: 18.6330\n",
      "Epoch 254/1000\n",
      "112247/112247 [==============================] - 2s - loss: 382.6688 - rmse_: 19.5203 - val_loss: 350.0435 - val_rmse_: 18.6463\n",
      "Epoch 255/1000\n",
      "112247/112247 [==============================] - 3s - loss: 382.1967 - rmse_: 19.5056 - val_loss: 349.9460 - val_rmse_: 18.6369\n",
      "Epoch 256/1000\n",
      "112247/112247 [==============================] - 3s - loss: 380.5995 - rmse_: 19.4648 - val_loss: 349.5912 - val_rmse_: 18.6322\n",
      "Epoch 257/1000\n",
      "112247/112247 [==============================] - 3s - loss: 380.8220 - rmse_: 19.4680 - val_loss: 349.1732 - val_rmse_: 18.6200\n",
      "Epoch 258/1000\n",
      "112247/112247 [==============================] - 3s - loss: 380.5181 - rmse_: 19.4664 - val_loss: 349.0535 - val_rmse_: 18.6176\n",
      "Epoch 259/1000\n",
      "112247/112247 [==============================] - 3s - loss: 380.4227 - rmse_: 19.4696 - val_loss: 348.5146 - val_rmse_: 18.5989\n",
      "Epoch 260/1000\n",
      "112247/112247 [==============================] - 3s - loss: 379.5509 - rmse_: 19.4398 - val_loss: 348.3475 - val_rmse_: 18.5938\n",
      "Epoch 261/1000\n",
      "112247/112247 [==============================] - 3s - loss: 380.0310 - rmse_: 19.4598 - val_loss: 348.2782 - val_rmse_: 18.5855\n",
      "Epoch 262/1000\n",
      "112247/112247 [==============================] - 3s - loss: 379.5389 - rmse_: 19.4430 - val_loss: 348.1728 - val_rmse_: 18.5933\n",
      "Epoch 263/1000\n",
      "112247/112247 [==============================] - 3s - loss: 379.5230 - rmse_: 19.4473 - val_loss: 347.7640 - val_rmse_: 18.5795\n",
      "Epoch 264/1000\n",
      "112247/112247 [==============================] - 3s - loss: 378.4528 - rmse_: 19.4113 - val_loss: 347.6000 - val_rmse_: 18.5797\n",
      "Epoch 265/1000\n",
      "112247/112247 [==============================] - 3s - loss: 379.4218 - rmse_: 19.4349 - val_loss: 347.6778 - val_rmse_: 18.5687\n",
      "Epoch 266/1000\n",
      "112247/112247 [==============================] - 3s - loss: 379.8970 - rmse_: 19.4491 - val_loss: 347.2703 - val_rmse_: 18.5690\n",
      "Epoch 267/1000\n",
      "112247/112247 [==============================] - 3s - loss: 381.6240 - rmse_: 19.4946 - val_loss: 347.1173 - val_rmse_: 18.5575\n",
      "Epoch 268/1000\n",
      "112247/112247 [==============================] - 3s - loss: 379.7748 - rmse_: 19.4503 - val_loss: 347.4425 - val_rmse_: 18.5820\n",
      "Epoch 269/1000\n",
      "112247/112247 [==============================] - 3s - loss: 377.5746 - rmse_: 19.3984 - val_loss: 347.1147 - val_rmse_: 18.5704\n",
      "Epoch 270/1000\n",
      "112247/112247 [==============================] - 3s - loss: 379.2150 - rmse_: 19.4377 - val_loss: 346.7681 - val_rmse_: 18.5584\n",
      "Epoch 271/1000\n",
      "112247/112247 [==============================] - 3s - loss: 377.8548 - rmse_: 19.3968 - val_loss: 346.3933 - val_rmse_: 18.5485\n",
      "Epoch 272/1000\n",
      "112247/112247 [==============================] - 3s - loss: 376.0860 - rmse_: 19.3570 - val_loss: 346.2779 - val_rmse_: 18.5431\n",
      "Epoch 273/1000\n",
      "112247/112247 [==============================] - 3s - loss: 375.6237 - rmse_: 19.3379 - val_loss: 345.6974 - val_rmse_: 18.5227\n",
      "Epoch 274/1000\n",
      "112247/112247 [==============================] - 3s - loss: 379.0882 - rmse_: 19.4306 - val_loss: 345.9253 - val_rmse_: 18.5338\n",
      "Epoch 275/1000\n",
      "112247/112247 [==============================] - 3s - loss: 377.4922 - rmse_: 19.3863 - val_loss: 345.3433 - val_rmse_: 18.5149\n",
      "Epoch 276/1000\n",
      "112247/112247 [==============================] - 3s - loss: 377.2091 - rmse_: 19.3810 - val_loss: 345.3895 - val_rmse_: 18.5116\n",
      "Epoch 277/1000\n",
      "112247/112247 [==============================] - 3s - loss: 375.2143 - rmse_: 19.3308 - val_loss: 345.0584 - val_rmse_: 18.5079\n",
      "Epoch 278/1000\n",
      "112247/112247 [==============================] - 3s - loss: 375.6577 - rmse_: 19.3432 - val_loss: 344.7824 - val_rmse_: 18.5024\n",
      "Epoch 279/1000\n",
      "112247/112247 [==============================] - 3s - loss: 376.8573 - rmse_: 19.3732 - val_loss: 344.9150 - val_rmse_: 18.5013\n",
      "Epoch 280/1000\n",
      "112247/112247 [==============================] - 3s - loss: 377.1866 - rmse_: 19.3727 - val_loss: 345.0753 - val_rmse_: 18.5124\n",
      "Epoch 281/1000\n",
      "112247/112247 [==============================] - 3s - loss: 376.7647 - rmse_: 19.3713 - val_loss: 344.5477 - val_rmse_: 18.4983\n",
      "Epoch 282/1000\n",
      "112247/112247 [==============================] - 3s - loss: 376.8103 - rmse_: 19.3655 - val_loss: 344.7202 - val_rmse_: 18.5063\n",
      "Epoch 283/1000\n",
      "112247/112247 [==============================] - 3s - loss: 374.5370 - rmse_: 19.3079 - val_loss: 344.2102 - val_rmse_: 18.4890\n",
      "Epoch 284/1000\n",
      "112247/112247 [==============================] - 3s - loss: 375.7343 - rmse_: 19.3442 - val_loss: 343.9342 - val_rmse_: 18.4855\n",
      "Epoch 285/1000\n",
      "112247/112247 [==============================] - 3s - loss: 376.1095 - rmse_: 19.3573 - val_loss: 343.6450 - val_rmse_: 18.4743\n",
      "Epoch 286/1000\n",
      "112247/112247 [==============================] - 3s - loss: 373.6436 - rmse_: 19.2879 - val_loss: 343.3715 - val_rmse_: 18.4638\n",
      "Epoch 287/1000\n",
      "112247/112247 [==============================] - 3s - loss: 375.1744 - rmse_: 19.3310 - val_loss: 343.1764 - val_rmse_: 18.4615\n",
      "Epoch 288/1000\n",
      "112247/112247 [==============================] - 3s - loss: 376.6188 - rmse_: 19.3670 - val_loss: 343.4915 - val_rmse_: 18.4748\n",
      "Epoch 289/1000\n",
      "112247/112247 [==============================] - 3s - loss: 373.9144 - rmse_: 19.2955 - val_loss: 342.8117 - val_rmse_: 18.4490\n",
      "Epoch 290/1000\n",
      "112247/112247 [==============================] - 3s - loss: 373.4787 - rmse_: 19.2885 - val_loss: 342.9192 - val_rmse_: 18.4537\n",
      "Epoch 291/1000\n",
      "112247/112247 [==============================] - 3s - loss: 372.1974 - rmse_: 19.2568 - val_loss: 342.4883 - val_rmse_: 18.4451\n",
      "Epoch 292/1000\n",
      "112247/112247 [==============================] - 3s - loss: 373.6407 - rmse_: 19.2902 - val_loss: 342.6937 - val_rmse_: 18.4441\n",
      "Epoch 293/1000\n",
      "112247/112247 [==============================] - 3s - loss: 374.3262 - rmse_: 19.3065 - val_loss: 342.2185 - val_rmse_: 18.4305\n",
      "Epoch 294/1000\n",
      "112247/112247 [==============================] - 3s - loss: 375.6574 - rmse_: 19.3338 - val_loss: 342.1714 - val_rmse_: 18.4246\n",
      "Epoch 295/1000\n",
      "112247/112247 [==============================] - 3s - loss: 374.6505 - rmse_: 19.3152 - val_loss: 342.0137 - val_rmse_: 18.4209\n",
      "Epoch 296/1000\n",
      "112247/112247 [==============================] - 3s - loss: 374.4381 - rmse_: 19.3125 - val_loss: 341.6821 - val_rmse_: 18.4228\n",
      "Epoch 297/1000\n",
      "112247/112247 [==============================] - 3s - loss: 372.7412 - rmse_: 19.2591 - val_loss: 341.4249 - val_rmse_: 18.4119\n",
      "Epoch 298/1000\n",
      "112247/112247 [==============================] - 3s - loss: 373.5053 - rmse_: 19.2828 - val_loss: 341.1192 - val_rmse_: 18.4073\n",
      "Epoch 299/1000\n",
      "112247/112247 [==============================] - 3s - loss: 372.8261 - rmse_: 19.2683 - val_loss: 341.2291 - val_rmse_: 18.4059\n",
      "Epoch 300/1000\n",
      "112247/112247 [==============================] - 3s - loss: 372.0221 - rmse_: 19.2461 - val_loss: 340.8585 - val_rmse_: 18.4016\n",
      "Epoch 301/1000\n",
      "112247/112247 [==============================] - 3s - loss: 373.1419 - rmse_: 19.2767 - val_loss: 340.9005 - val_rmse_: 18.4026\n",
      "Epoch 302/1000\n",
      "112247/112247 [==============================] - 3s - loss: 372.4101 - rmse_: 19.2579 - val_loss: 340.5975 - val_rmse_: 18.3962\n",
      "Epoch 303/1000\n",
      "112247/112247 [==============================] - 3s - loss: 371.8474 - rmse_: 19.2459 - val_loss: 340.2921 - val_rmse_: 18.3808\n",
      "Epoch 304/1000\n",
      "112247/112247 [==============================] - 3s - loss: 372.0471 - rmse_: 19.2475 - val_loss: 340.4021 - val_rmse_: 18.3861\n",
      "Epoch 305/1000\n",
      "112247/112247 [==============================] - 3s - loss: 370.4834 - rmse_: 19.2069 - val_loss: 339.9862 - val_rmse_: 18.3758\n",
      "Epoch 306/1000\n",
      "112247/112247 [==============================] - 3s - loss: 370.2039 - rmse_: 19.1978 - val_loss: 339.7967 - val_rmse_: 18.3673\n",
      "Epoch 307/1000\n",
      "112247/112247 [==============================] - 3s - loss: 371.5927 - rmse_: 19.2399 - val_loss: 339.8279 - val_rmse_: 18.3653\n",
      "Epoch 308/1000\n",
      "112247/112247 [==============================] - 3s - loss: 372.1004 - rmse_: 19.2439 - val_loss: 339.7292 - val_rmse_: 18.3681\n",
      "Epoch 309/1000\n",
      "112247/112247 [==============================] - 3s - loss: 371.5668 - rmse_: 19.2354 - val_loss: 339.4203 - val_rmse_: 18.3576\n",
      "Epoch 310/1000\n",
      "112247/112247 [==============================] - 3s - loss: 371.4024 - rmse_: 19.2344 - val_loss: 339.3069 - val_rmse_: 18.3559\n",
      "Epoch 311/1000\n",
      "112247/112247 [==============================] - 3s - loss: 371.6792 - rmse_: 19.2381 - val_loss: 339.2372 - val_rmse_: 18.3576\n",
      "Epoch 312/1000\n",
      "112247/112247 [==============================] - 3s - loss: 371.0342 - rmse_: 19.2212 - val_loss: 338.8940 - val_rmse_: 18.3440\n",
      "Epoch 313/1000\n",
      "112247/112247 [==============================] - 3s - loss: 368.8993 - rmse_: 19.1637 - val_loss: 338.7330 - val_rmse_: 18.3468\n",
      "Epoch 314/1000\n",
      "112247/112247 [==============================] - 3s - loss: 371.3001 - rmse_: 19.2299 - val_loss: 338.8069 - val_rmse_: 18.3460\n",
      "Epoch 315/1000\n",
      "112247/112247 [==============================] - 3s - loss: 370.8410 - rmse_: 19.2182 - val_loss: 338.6526 - val_rmse_: 18.3389\n",
      "Epoch 316/1000\n",
      "112247/112247 [==============================] - 3s - loss: 369.4892 - rmse_: 19.1854 - val_loss: 338.4442 - val_rmse_: 18.3296\n",
      "Epoch 317/1000\n",
      "112247/112247 [==============================] - 3s - loss: 370.4251 - rmse_: 19.2055 - val_loss: 338.3014 - val_rmse_: 18.3275\n",
      "Epoch 318/1000\n",
      "112247/112247 [==============================] - 3s - loss: 368.8905 - rmse_: 19.1637 - val_loss: 337.9299 - val_rmse_: 18.3214\n",
      "Epoch 319/1000\n",
      "112247/112247 [==============================] - 3s - loss: 367.0587 - rmse_: 19.1130 - val_loss: 337.8285 - val_rmse_: 18.3188\n",
      "Epoch 320/1000\n",
      "112247/112247 [==============================] - 3s - loss: 369.4845 - rmse_: 19.1815 - val_loss: 337.4592 - val_rmse_: 18.3083\n",
      "Epoch 321/1000\n",
      "112247/112247 [==============================] - 3s - loss: 366.8621 - rmse_: 19.1152 - val_loss: 337.4867 - val_rmse_: 18.3084\n",
      "Epoch 322/1000\n",
      "112247/112247 [==============================] - 3s - loss: 368.4836 - rmse_: 19.1555 - val_loss: 337.4825 - val_rmse_: 18.3018\n",
      "Epoch 323/1000\n",
      "112247/112247 [==============================] - 3s - loss: 368.9415 - rmse_: 19.1695 - val_loss: 337.2510 - val_rmse_: 18.3045\n",
      "Epoch 324/1000\n",
      "112247/112247 [==============================] - 3s - loss: 369.0867 - rmse_: 19.1749 - val_loss: 337.0722 - val_rmse_: 18.2980\n",
      "Epoch 325/1000\n",
      "112247/112247 [==============================] - 3s - loss: 368.2855 - rmse_: 19.1492 - val_loss: 337.0376 - val_rmse_: 18.2981\n",
      "Epoch 326/1000\n",
      "112247/112247 [==============================] - 3s - loss: 368.9758 - rmse_: 19.1705 - val_loss: 336.7657 - val_rmse_: 18.2927\n",
      "Epoch 327/1000\n",
      "112247/112247 [==============================] - 3s - loss: 366.7872 - rmse_: 19.1122 - val_loss: 336.3502 - val_rmse_: 18.2815\n",
      "Epoch 328/1000\n",
      "112247/112247 [==============================] - 3s - loss: 367.7905 - rmse_: 19.1388 - val_loss: 336.5726 - val_rmse_: 18.2877\n",
      "Epoch 329/1000\n",
      "112247/112247 [==============================] - 3s - loss: 367.7368 - rmse_: 19.1346 - val_loss: 336.2139 - val_rmse_: 18.2781\n",
      "Epoch 330/1000\n",
      "112247/112247 [==============================] - 3s - loss: 365.2686 - rmse_: 19.0729 - val_loss: 335.7917 - val_rmse_: 18.2640\n",
      "Epoch 331/1000\n",
      "112247/112247 [==============================] - 3s - loss: 367.0065 - rmse_: 19.1136 - val_loss: 335.5268 - val_rmse_: 18.2542\n",
      "Epoch 332/1000\n",
      "112247/112247 [==============================] - 3s - loss: 366.6680 - rmse_: 19.1063 - val_loss: 335.5903 - val_rmse_: 18.2626\n",
      "Epoch 333/1000\n",
      "112247/112247 [==============================] - 3s - loss: 365.4652 - rmse_: 19.0765 - val_loss: 335.3591 - val_rmse_: 18.2497\n",
      "Epoch 334/1000\n",
      "112247/112247 [==============================] - 3s - loss: 365.8890 - rmse_: 19.0812 - val_loss: 335.3312 - val_rmse_: 18.2491\n",
      "Epoch 335/1000\n",
      "112247/112247 [==============================] - 3s - loss: 366.3897 - rmse_: 19.1023 - val_loss: 335.1166 - val_rmse_: 18.2472\n",
      "Epoch 336/1000\n",
      "112247/112247 [==============================] - 3s - loss: 367.2331 - rmse_: 19.1256 - val_loss: 335.5550 - val_rmse_: 18.2501\n",
      "Epoch 337/1000\n",
      "112247/112247 [==============================] - 3s - loss: 367.0475 - rmse_: 19.1225 - val_loss: 335.2395 - val_rmse_: 18.2507\n",
      "Epoch 338/1000\n",
      "112247/112247 [==============================] - 3s - loss: 366.9751 - rmse_: 19.1200 - val_loss: 335.0035 - val_rmse_: 18.2418\n",
      "Epoch 339/1000\n",
      "112247/112247 [==============================] - 3s - loss: 366.4949 - rmse_: 19.1050 - val_loss: 335.0442 - val_rmse_: 18.2410\n",
      "Epoch 340/1000\n",
      "112247/112247 [==============================] - 3s - loss: 364.6415 - rmse_: 19.0562 - val_loss: 334.5340 - val_rmse_: 18.2265\n",
      "Epoch 341/1000\n",
      "112247/112247 [==============================] - 3s - loss: 367.2175 - rmse_: 19.1288 - val_loss: 334.8653 - val_rmse_: 18.2363\n",
      "Epoch 342/1000\n",
      "112247/112247 [==============================] - 3s - loss: 365.0286 - rmse_: 19.0670 - val_loss: 334.2313 - val_rmse_: 18.2252\n",
      "Epoch 343/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112247/112247 [==============================] - 3s - loss: 366.0718 - rmse_: 19.0872 - val_loss: 334.4771 - val_rmse_: 18.2356\n",
      "Epoch 344/1000\n",
      "112247/112247 [==============================] - 3s - loss: 366.1124 - rmse_: 19.0927 - val_loss: 334.0105 - val_rmse_: 18.2170\n",
      "Epoch 345/1000\n",
      "112247/112247 [==============================] - 3s - loss: 364.7475 - rmse_: 19.0523 - val_loss: 334.1559 - val_rmse_: 18.2224\n",
      "Epoch 346/1000\n",
      "112247/112247 [==============================] - 3s - loss: 365.7992 - rmse_: 19.0810 - val_loss: 333.9834 - val_rmse_: 18.2163\n",
      "Epoch 347/1000\n",
      "112247/112247 [==============================] - 3s - loss: 363.5915 - rmse_: 19.0304 - val_loss: 333.7319 - val_rmse_: 18.2090\n",
      "Epoch 348/1000\n",
      "112247/112247 [==============================] - 3s - loss: 363.9587 - rmse_: 19.0448 - val_loss: 333.3824 - val_rmse_: 18.2013\n",
      "Epoch 349/1000\n",
      "112247/112247 [==============================] - 3s - loss: 365.1494 - rmse_: 19.0704 - val_loss: 333.1666 - val_rmse_: 18.1935\n",
      "Epoch 350/1000\n",
      "112247/112247 [==============================] - 3s - loss: 366.3009 - rmse_: 19.1032 - val_loss: 333.3332 - val_rmse_: 18.2019\n",
      "Epoch 351/1000\n",
      "112247/112247 [==============================] - 3s - loss: 364.9572 - rmse_: 19.0594 - val_loss: 333.1233 - val_rmse_: 18.1919\n",
      "Epoch 352/1000\n",
      "112247/112247 [==============================] - 3s - loss: 364.0992 - rmse_: 19.0431 - val_loss: 333.1897 - val_rmse_: 18.1979\n",
      "Epoch 353/1000\n",
      "112247/112247 [==============================] - 3s - loss: 364.9919 - rmse_: 19.0623 - val_loss: 333.3480 - val_rmse_: 18.1950\n",
      "Epoch 354/1000\n",
      "112247/112247 [==============================] - 3s - loss: 364.6888 - rmse_: 19.0563 - val_loss: 332.4804 - val_rmse_: 18.1771\n",
      "Epoch 355/1000\n",
      "112247/112247 [==============================] - 3s - loss: 361.8509 - rmse_: 18.9763 - val_loss: 332.8384 - val_rmse_: 18.1792\n",
      "Epoch 356/1000\n",
      "112247/112247 [==============================] - 3s - loss: 364.8472 - rmse_: 19.0626 - val_loss: 332.2727 - val_rmse_: 18.1676\n",
      "Epoch 357/1000\n",
      "112247/112247 [==============================] - 3s - loss: 364.5273 - rmse_: 19.0514 - val_loss: 332.3299 - val_rmse_: 18.1766\n",
      "Epoch 358/1000\n",
      "112247/112247 [==============================] - 3s - loss: 362.8408 - rmse_: 19.0046 - val_loss: 332.1008 - val_rmse_: 18.1663\n",
      "Epoch 359/1000\n",
      "112247/112247 [==============================] - 3s - loss: 362.6749 - rmse_: 19.0042 - val_loss: 332.0271 - val_rmse_: 18.1612\n",
      "Epoch 360/1000\n",
      "112247/112247 [==============================] - 3s - loss: 362.7090 - rmse_: 19.0100 - val_loss: 332.0728 - val_rmse_: 18.1588\n",
      "Epoch 361/1000\n",
      "112247/112247 [==============================] - 3s - loss: 363.5075 - rmse_: 19.0248 - val_loss: 331.7307 - val_rmse_: 18.1568\n",
      "Epoch 362/1000\n",
      "112247/112247 [==============================] - 3s - loss: 363.9211 - rmse_: 19.0388 - val_loss: 331.6898 - val_rmse_: 18.1568\n",
      "Epoch 363/1000\n",
      "112247/112247 [==============================] - 3s - loss: 361.9500 - rmse_: 18.9912 - val_loss: 331.4048 - val_rmse_: 18.1485\n",
      "Epoch 364/1000\n",
      "112247/112247 [==============================] - 3s - loss: 362.1261 - rmse_: 18.9869 - val_loss: 331.3797 - val_rmse_: 18.1484\n",
      "Epoch 365/1000\n",
      "112247/112247 [==============================] - 3s - loss: 364.5252 - rmse_: 19.0516 - val_loss: 331.1062 - val_rmse_: 18.1417\n",
      "Epoch 366/1000\n",
      "112247/112247 [==============================] - 3s - loss: 363.0828 - rmse_: 19.0073 - val_loss: 331.0235 - val_rmse_: 18.1323\n",
      "Epoch 367/1000\n",
      "112247/112247 [==============================] - 3s - loss: 361.0674 - rmse_: 18.9586 - val_loss: 330.7674 - val_rmse_: 18.1293\n",
      "Epoch 368/1000\n",
      "112247/112247 [==============================] - 3s - loss: 363.1268 - rmse_: 19.0082 - val_loss: 330.4994 - val_rmse_: 18.1224\n",
      "Epoch 369/1000\n",
      "112247/112247 [==============================] - 3s - loss: 360.3762 - rmse_: 18.9383 - val_loss: 330.4732 - val_rmse_: 18.1209\n",
      "Epoch 370/1000\n",
      "112247/112247 [==============================] - 3s - loss: 362.0487 - rmse_: 18.9820 - val_loss: 330.3069 - val_rmse_: 18.1186\n",
      "Epoch 371/1000\n",
      "112247/112247 [==============================] - 3s - loss: 360.2956 - rmse_: 18.9459 - val_loss: 330.2633 - val_rmse_: 18.1121\n",
      "Epoch 372/1000\n",
      "112247/112247 [==============================] - 3s - loss: 360.6474 - rmse_: 18.9384 - val_loss: 329.8135 - val_rmse_: 18.1051\n",
      "Epoch 373/1000\n",
      "112247/112247 [==============================] - 3s - loss: 358.2664 - rmse_: 18.8884 - val_loss: 329.6651 - val_rmse_: 18.1004\n",
      "Epoch 374/1000\n",
      "112247/112247 [==============================] - 2s - loss: 359.8933 - rmse_: 18.9257 - val_loss: 329.9330 - val_rmse_: 18.1048\n",
      "Epoch 375/1000\n",
      "112247/112247 [==============================] - 3s - loss: 360.5711 - rmse_: 18.9458 - val_loss: 329.6910 - val_rmse_: 18.1013\n",
      "Epoch 376/1000\n",
      "112247/112247 [==============================] - 3s - loss: 362.9213 - rmse_: 19.0109 - val_loss: 329.5081 - val_rmse_: 18.0992\n",
      "Epoch 377/1000\n",
      "112247/112247 [==============================] - 3s - loss: 360.0308 - rmse_: 18.9410 - val_loss: 329.5273 - val_rmse_: 18.0949\n",
      "Epoch 378/1000\n",
      "112247/112247 [==============================] - 2s - loss: 360.9650 - rmse_: 18.9550 - val_loss: 329.3120 - val_rmse_: 18.0893\n",
      "Epoch 379/1000\n",
      "112247/112247 [==============================] - 3s - loss: 359.4495 - rmse_: 18.9238 - val_loss: 329.1783 - val_rmse_: 18.0910\n",
      "Epoch 380/1000\n",
      "112247/112247 [==============================] - 3s - loss: 359.8754 - rmse_: 18.9285 - val_loss: 328.9136 - val_rmse_: 18.0821\n",
      "Epoch 381/1000\n",
      "112247/112247 [==============================] - 3s - loss: 359.7661 - rmse_: 18.9284 - val_loss: 328.9242 - val_rmse_: 18.0835\n",
      "Epoch 382/1000\n",
      "112247/112247 [==============================] - 3s - loss: 359.3685 - rmse_: 18.9235 - val_loss: 328.7248 - val_rmse_: 18.0761\n",
      "Epoch 383/1000\n",
      "112247/112247 [==============================] - 3s - loss: 359.1462 - rmse_: 18.9036 - val_loss: 328.7687 - val_rmse_: 18.0781\n",
      "Epoch 384/1000\n",
      "112247/112247 [==============================] - 3s - loss: 359.0196 - rmse_: 18.9026 - val_loss: 328.9059 - val_rmse_: 18.0767\n",
      "Epoch 385/1000\n",
      "112247/112247 [==============================] - 3s - loss: 363.2544 - rmse_: 19.0153 - val_loss: 328.7678 - val_rmse_: 18.0731\n",
      "Epoch 386/1000\n",
      "112247/112247 [==============================] - 3s - loss: 360.6593 - rmse_: 18.9560 - val_loss: 328.4673 - val_rmse_: 18.0700\n",
      "Epoch 387/1000\n",
      "112247/112247 [==============================] - 3s - loss: 359.1190 - rmse_: 18.9113 - val_loss: 328.2320 - val_rmse_: 18.0583\n",
      "Epoch 388/1000\n",
      "112247/112247 [==============================] - 3s - loss: 358.7618 - rmse_: 18.9037 - val_loss: 327.8811 - val_rmse_: 18.0490\n",
      "Epoch 389/1000\n",
      "112247/112247 [==============================] - 3s - loss: 358.4659 - rmse_: 18.8872 - val_loss: 327.9205 - val_rmse_: 18.0506\n",
      "Epoch 390/1000\n",
      "112247/112247 [==============================] - 3s - loss: 359.1484 - rmse_: 18.9111 - val_loss: 327.7757 - val_rmse_: 18.0513\n",
      "Epoch 391/1000\n",
      "112247/112247 [==============================] - 3s - loss: 359.2816 - rmse_: 18.9214 - val_loss: 327.9427 - val_rmse_: 18.0487\n",
      "Epoch 392/1000\n",
      "112247/112247 [==============================] - 3s - loss: 359.1122 - rmse_: 18.9121 - val_loss: 327.7131 - val_rmse_: 18.0439\n",
      "Epoch 393/1000\n",
      "112247/112247 [==============================] - 3s - loss: 359.7533 - rmse_: 18.9292 - val_loss: 327.1903 - val_rmse_: 18.0300\n",
      "Epoch 394/1000\n",
      "112247/112247 [==============================] - 3s - loss: 357.3425 - rmse_: 18.8669 - val_loss: 327.1248 - val_rmse_: 18.0354\n",
      "Epoch 395/1000\n",
      "112247/112247 [==============================] - 3s - loss: 357.4105 - rmse_: 18.8623 - val_loss: 327.0851 - val_rmse_: 18.0305\n",
      "Epoch 396/1000\n",
      "112247/112247 [==============================] - 3s - loss: 355.2839 - rmse_: 18.8117 - val_loss: 327.1847 - val_rmse_: 18.0287\n",
      "Epoch 397/1000\n",
      "112247/112247 [==============================] - 3s - loss: 356.6064 - rmse_: 18.8450 - val_loss: 327.0296 - val_rmse_: 18.0246\n",
      "Epoch 398/1000\n",
      "112247/112247 [==============================] - 3s - loss: 357.5967 - rmse_: 18.8714 - val_loss: 326.8716 - val_rmse_: 18.0266\n",
      "Epoch 399/1000\n",
      "112247/112247 [==============================] - 3s - loss: 359.1048 - rmse_: 18.9139 - val_loss: 326.7575 - val_rmse_: 18.0197\n",
      "Epoch 400/1000\n",
      "112247/112247 [==============================] - 3s - loss: 359.2912 - rmse_: 18.9037 - val_loss: 326.9129 - val_rmse_: 18.0187\n",
      "Epoch 401/1000\n",
      "112247/112247 [==============================] - 3s - loss: 359.0283 - rmse_: 18.9047 - val_loss: 326.6074 - val_rmse_: 18.0172\n",
      "Epoch 402/1000\n",
      "112247/112247 [==============================] - 3s - loss: 357.0119 - rmse_: 18.8540 - val_loss: 326.3935 - val_rmse_: 18.0089\n",
      "Epoch 403/1000\n",
      "112247/112247 [==============================] - 3s - loss: 358.0323 - rmse_: 18.8778 - val_loss: 326.0141 - val_rmse_: 18.0013\n",
      "Epoch 404/1000\n",
      "112247/112247 [==============================] - 3s - loss: 355.9157 - rmse_: 18.8243 - val_loss: 326.1006 - val_rmse_: 18.0035\n",
      "Epoch 405/1000\n",
      "112247/112247 [==============================] - 3s - loss: 355.2123 - rmse_: 18.8025 - val_loss: 326.3386 - val_rmse_: 18.0090\n",
      "Epoch 406/1000\n",
      "112247/112247 [==============================] - 3s - loss: 356.0255 - rmse_: 18.8231 - val_loss: 326.1438 - val_rmse_: 18.0027\n",
      "Epoch 407/1000\n",
      "112247/112247 [==============================] - 3s - loss: 357.3100 - rmse_: 18.8605 - val_loss: 326.0850 - val_rmse_: 18.0031\n",
      "Epoch 408/1000\n",
      "112247/112247 [==============================] - 3s - loss: 356.4301 - rmse_: 18.8323 - val_loss: 325.7734 - val_rmse_: 17.9937\n",
      "Epoch 409/1000\n",
      "112247/112247 [==============================] - 3s - loss: 356.1402 - rmse_: 18.8348 - val_loss: 325.6365 - val_rmse_: 17.9898\n",
      "Epoch 410/1000\n",
      "112247/112247 [==============================] - 3s - loss: 355.8543 - rmse_: 18.8298 - val_loss: 325.3243 - val_rmse_: 17.9851\n",
      "Epoch 411/1000\n",
      "112247/112247 [==============================] - 3s - loss: 357.6255 - rmse_: 18.8677 - val_loss: 325.2323 - val_rmse_: 17.9824\n",
      "Epoch 412/1000\n",
      "112247/112247 [==============================] - 3s - loss: 355.6301 - rmse_: 18.8179 - val_loss: 325.1660 - val_rmse_: 17.9816\n",
      "Epoch 413/1000\n",
      "112247/112247 [==============================] - 3s - loss: 356.3947 - rmse_: 18.8428 - val_loss: 325.3696 - val_rmse_: 17.9808\n",
      "Epoch 414/1000\n",
      "112247/112247 [==============================] - 3s - loss: 356.9293 - rmse_: 18.8551 - val_loss: 325.1975 - val_rmse_: 17.9769\n",
      "Epoch 415/1000\n",
      "112247/112247 [==============================] - 3s - loss: 354.2601 - rmse_: 18.7819 - val_loss: 324.9108 - val_rmse_: 17.9710\n",
      "Epoch 416/1000\n",
      "112247/112247 [==============================] - 3s - loss: 355.6962 - rmse_: 18.8252 - val_loss: 325.1239 - val_rmse_: 17.9716\n",
      "Epoch 417/1000\n",
      "112247/112247 [==============================] - 3s - loss: 356.5482 - rmse_: 18.8473 - val_loss: 324.7236 - val_rmse_: 17.9642\n",
      "Epoch 418/1000\n",
      "112247/112247 [==============================] - 3s - loss: 354.9662 - rmse_: 18.8012 - val_loss: 324.4948 - val_rmse_: 17.9602\n",
      "Epoch 419/1000\n",
      "112247/112247 [==============================] - 3s - loss: 352.6132 - rmse_: 18.7371 - val_loss: 324.0920 - val_rmse_: 17.9507\n",
      "Epoch 420/1000\n",
      "112247/112247 [==============================] - 3s - loss: 357.4034 - rmse_: 18.8652 - val_loss: 324.0508 - val_rmse_: 17.9495\n",
      "Epoch 421/1000\n",
      "112247/112247 [==============================] - 3s - loss: 353.3274 - rmse_: 18.7542 - val_loss: 324.0028 - val_rmse_: 17.9486\n",
      "Epoch 422/1000\n",
      "112247/112247 [==============================] - 3s - loss: 355.6812 - rmse_: 18.8138 - val_loss: 324.0750 - val_rmse_: 17.9472\n",
      "Epoch 423/1000\n",
      "112247/112247 [==============================] - 3s - loss: 355.4569 - rmse_: 18.8179 - val_loss: 324.2070 - val_rmse_: 17.9484\n",
      "Epoch 424/1000\n",
      "112247/112247 [==============================] - 3s - loss: 353.8439 - rmse_: 18.7668 - val_loss: 323.9345 - val_rmse_: 17.9425\n",
      "Epoch 425/1000\n",
      "112247/112247 [==============================] - 3s - loss: 356.1750 - rmse_: 18.8363 - val_loss: 323.8685 - val_rmse_: 17.9395\n",
      "Epoch 426/1000\n",
      "112247/112247 [==============================] - 3s - loss: 356.9447 - rmse_: 18.8565 - val_loss: 323.6914 - val_rmse_: 17.9377\n",
      "Epoch 427/1000\n",
      "112247/112247 [==============================] - 3s - loss: 355.6034 - rmse_: 18.8168 - val_loss: 323.3734 - val_rmse_: 17.9324\n",
      "Epoch 428/1000\n",
      "112247/112247 [==============================] - 3s - loss: 354.6805 - rmse_: 18.7906 - val_loss: 323.2955 - val_rmse_: 17.9283\n",
      "Epoch 429/1000\n",
      "112247/112247 [==============================] - 2s - loss: 354.1693 - rmse_: 18.7773 - val_loss: 323.3928 - val_rmse_: 17.9301\n",
      "Epoch 430/1000\n",
      "112247/112247 [==============================] - 3s - loss: 354.3469 - rmse_: 18.7848 - val_loss: 323.0546 - val_rmse_: 17.9236\n",
      "Epoch 431/1000\n",
      "112247/112247 [==============================] - 3s - loss: 354.9401 - rmse_: 18.7959 - val_loss: 323.3783 - val_rmse_: 17.9285\n",
      "Epoch 432/1000\n",
      "112247/112247 [==============================] - 3s - loss: 354.4875 - rmse_: 18.7940 - val_loss: 322.8517 - val_rmse_: 17.9177\n",
      "Epoch 433/1000\n",
      "112247/112247 [==============================] - 3s - loss: 352.7923 - rmse_: 18.7446 - val_loss: 322.8923 - val_rmse_: 17.9183\n",
      "Epoch 434/1000\n",
      "112247/112247 [==============================] - 3s - loss: 354.5378 - rmse_: 18.7909 - val_loss: 322.7117 - val_rmse_: 17.9097\n",
      "Epoch 435/1000\n",
      "112247/112247 [==============================] - 3s - loss: 354.8737 - rmse_: 18.7991 - val_loss: 322.6419 - val_rmse_: 17.9058\n",
      "Epoch 436/1000\n",
      "112247/112247 [==============================] - 3s - loss: 353.3091 - rmse_: 18.7598 - val_loss: 322.2371 - val_rmse_: 17.9000\n",
      "Epoch 437/1000\n",
      "112247/112247 [==============================] - 3s - loss: 353.6680 - rmse_: 18.7644 - val_loss: 322.6400 - val_rmse_: 17.9052\n",
      "Epoch 438/1000\n",
      "112247/112247 [==============================] - 3s - loss: 351.0480 - rmse_: 18.6932 - val_loss: 322.2517 - val_rmse_: 17.8987\n",
      "Epoch 439/1000\n",
      "112247/112247 [==============================] - 3s - loss: 353.7623 - rmse_: 18.7736 - val_loss: 322.2578 - val_rmse_: 17.8984\n",
      "Epoch 440/1000\n",
      "112247/112247 [==============================] - 3s - loss: 352.9496 - rmse_: 18.7489 - val_loss: 321.7768 - val_rmse_: 17.8864\n",
      "Epoch 441/1000\n",
      "112247/112247 [==============================] - 3s - loss: 353.1441 - rmse_: 18.7483 - val_loss: 322.1088 - val_rmse_: 17.8905\n",
      "Epoch 442/1000\n",
      "112247/112247 [==============================] - 3s - loss: 354.2378 - rmse_: 18.7714 - val_loss: 322.1209 - val_rmse_: 17.8900\n",
      "Epoch 443/1000\n",
      "112247/112247 [==============================] - 3s - loss: 352.8865 - rmse_: 18.7373 - val_loss: 322.1088 - val_rmse_: 17.8878\n",
      "Epoch 444/1000\n",
      "112247/112247 [==============================] - 3s - loss: 352.8892 - rmse_: 18.7500 - val_loss: 321.8691 - val_rmse_: 17.8850\n",
      "Epoch 445/1000\n",
      "112247/112247 [==============================] - 3s - loss: 353.2337 - rmse_: 18.7558 - val_loss: 321.6715 - val_rmse_: 17.8789\n",
      "Epoch 446/1000\n",
      "112247/112247 [==============================] - 3s - loss: 350.8564 - rmse_: 18.6907 - val_loss: 321.4132 - val_rmse_: 17.8774\n",
      "Epoch 447/1000\n",
      "112247/112247 [==============================] - 3s - loss: 352.9954 - rmse_: 18.7541 - val_loss: 321.4339 - val_rmse_: 17.8763\n",
      "Epoch 448/1000\n",
      "112247/112247 [==============================] - 3s - loss: 352.5535 - rmse_: 18.7373 - val_loss: 321.5533 - val_rmse_: 17.8768\n",
      "Epoch 449/1000\n",
      "112247/112247 [==============================] - 3s - loss: 352.2487 - rmse_: 18.7295 - val_loss: 321.0290 - val_rmse_: 17.8674\n",
      "Epoch 450/1000\n",
      "112247/112247 [==============================] - 3s - loss: 351.8767 - rmse_: 18.7173 - val_loss: 321.2548 - val_rmse_: 17.8705\n",
      "Epoch 451/1000\n",
      "112247/112247 [==============================] - 3s - loss: 352.7204 - rmse_: 18.7463 - val_loss: 321.0697 - val_rmse_: 17.8649\n",
      "Epoch 452/1000\n",
      "112247/112247 [==============================] - 3s - loss: 353.0874 - rmse_: 18.7487 - val_loss: 321.3998 - val_rmse_: 17.8717\n",
      "Epoch 453/1000\n",
      "112247/112247 [==============================] - 3s - loss: 350.4695 - rmse_: 18.6792 - val_loss: 320.9021 - val_rmse_: 17.8614\n",
      "Epoch 454/1000\n",
      "112247/112247 [==============================] - 3s - loss: 350.1443 - rmse_: 18.6699 - val_loss: 320.7013 - val_rmse_: 17.8576\n",
      "Epoch 455/1000\n",
      "112247/112247 [==============================] - 3s - loss: 350.3774 - rmse_: 18.6728 - val_loss: 320.6785 - val_rmse_: 17.8564\n",
      "Epoch 456/1000\n",
      "112247/112247 [==============================] - 3s - loss: 350.3547 - rmse_: 18.6687 - val_loss: 320.5507 - val_rmse_: 17.8514\n",
      "Epoch 457/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112247/112247 [==============================] - 3s - loss: 351.2534 - rmse_: 18.6990 - val_loss: 320.2827 - val_rmse_: 17.8474\n",
      "Epoch 458/1000\n",
      "112247/112247 [==============================] - 3s - loss: 351.2548 - rmse_: 18.7019 - val_loss: 320.4797 - val_rmse_: 17.8462\n",
      "Epoch 459/1000\n",
      "112247/112247 [==============================] - 3s - loss: 350.7205 - rmse_: 18.6829 - val_loss: 319.9236 - val_rmse_: 17.8383\n",
      "Epoch 460/1000\n",
      "112247/112247 [==============================] - 3s - loss: 352.6424 - rmse_: 18.7288 - val_loss: 320.0673 - val_rmse_: 17.8391\n",
      "Epoch 461/1000\n",
      "112247/112247 [==============================] - 3s - loss: 349.6829 - rmse_: 18.6655 - val_loss: 319.7045 - val_rmse_: 17.8319\n",
      "Epoch 462/1000\n",
      "112247/112247 [==============================] - 3s - loss: 351.1640 - rmse_: 18.6879 - val_loss: 319.6686 - val_rmse_: 17.8261\n",
      "Epoch 463/1000\n",
      "112247/112247 [==============================] - 3s - loss: 350.1463 - rmse_: 18.6730 - val_loss: 319.6623 - val_rmse_: 17.8248\n",
      "Epoch 464/1000\n",
      "112247/112247 [==============================] - 3s - loss: 351.1446 - rmse_: 18.7024 - val_loss: 319.4990 - val_rmse_: 17.8223\n",
      "Epoch 465/1000\n",
      "112247/112247 [==============================] - 3s - loss: 350.3948 - rmse_: 18.6732 - val_loss: 319.4356 - val_rmse_: 17.8212\n",
      "Epoch 466/1000\n",
      "112247/112247 [==============================] - 3s - loss: 351.4215 - rmse_: 18.7037 - val_loss: 319.4851 - val_rmse_: 17.8194\n",
      "Epoch 467/1000\n",
      "112247/112247 [==============================] - 3s - loss: 349.2036 - rmse_: 18.6450 - val_loss: 319.1558 - val_rmse_: 17.8154\n",
      "Epoch 468/1000\n",
      "112247/112247 [==============================] - 3s - loss: 350.7443 - rmse_: 18.6953 - val_loss: 319.0942 - val_rmse_: 17.8163\n",
      "Epoch 469/1000\n",
      "112247/112247 [==============================] - 3s - loss: 348.8411 - rmse_: 18.6406 - val_loss: 319.0720 - val_rmse_: 17.8124\n",
      "Epoch 470/1000\n",
      "112247/112247 [==============================] - 3s - loss: 352.1146 - rmse_: 18.7196 - val_loss: 319.4166 - val_rmse_: 17.8184\n",
      "Epoch 471/1000\n",
      "112247/112247 [==============================] - 3s - loss: 349.4694 - rmse_: 18.6489 - val_loss: 319.2085 - val_rmse_: 17.8145\n",
      "Epoch 472/1000\n",
      "112247/112247 [==============================] - 3s - loss: 350.1708 - rmse_: 18.6716 - val_loss: 318.7868 - val_rmse_: 17.8052\n",
      "Epoch 473/1000\n",
      "112247/112247 [==============================] - 3s - loss: 348.0654 - rmse_: 18.6175 - val_loss: 318.5189 - val_rmse_: 17.7999\n",
      "Epoch 474/1000\n",
      "112247/112247 [==============================] - 3s - loss: 349.4228 - rmse_: 18.6571 - val_loss: 318.4523 - val_rmse_: 17.7948\n",
      "Epoch 475/1000\n",
      "112247/112247 [==============================] - 3s - loss: 348.7729 - rmse_: 18.6373 - val_loss: 318.4111 - val_rmse_: 17.7928\n",
      "Epoch 476/1000\n",
      "112247/112247 [==============================] - 3s - loss: 352.1679 - rmse_: 18.7217 - val_loss: 319.0187 - val_rmse_: 17.8034\n",
      "Epoch 477/1000\n",
      "112247/112247 [==============================] - 3s - loss: 347.8567 - rmse_: 18.6100 - val_loss: 318.1196 - val_rmse_: 17.7864\n",
      "Epoch 478/1000\n",
      "112247/112247 [==============================] - 3s - loss: 350.6128 - rmse_: 18.6830 - val_loss: 318.1860 - val_rmse_: 17.7849\n",
      "Epoch 479/1000\n",
      "112247/112247 [==============================] - 3s - loss: 347.8762 - rmse_: 18.6044 - val_loss: 317.9118 - val_rmse_: 17.7768\n",
      "Epoch 480/1000\n",
      "112247/112247 [==============================] - 3s - loss: 351.6470 - rmse_: 18.7099 - val_loss: 318.0207 - val_rmse_: 17.7809\n",
      "Epoch 481/1000\n",
      "112247/112247 [==============================] - 3s - loss: 345.1384 - rmse_: 18.5458 - val_loss: 318.0685 - val_rmse_: 17.7817\n",
      "Epoch 482/1000\n",
      "112247/112247 [==============================] - 3s - loss: 349.4007 - rmse_: 18.6465 - val_loss: 318.1898 - val_rmse_: 17.7805\n",
      "Epoch 483/1000\n",
      "112247/112247 [==============================] - 3s - loss: 347.8328 - rmse_: 18.6022 - val_loss: 317.5570 - val_rmse_: 17.7710\n",
      "Epoch 484/1000\n",
      "112247/112247 [==============================] - 3s - loss: 348.5780 - rmse_: 18.6306 - val_loss: 317.6724 - val_rmse_: 17.7715\n",
      "Epoch 485/1000\n",
      "112247/112247 [==============================] - 3s - loss: 347.9717 - rmse_: 18.6129 - val_loss: 317.5263 - val_rmse_: 17.7679\n",
      "Epoch 486/1000\n",
      "112247/112247 [==============================] - 3s - loss: 347.1030 - rmse_: 18.5918 - val_loss: 317.5964 - val_rmse_: 17.7692\n",
      "Epoch 487/1000\n",
      "112247/112247 [==============================] - 3s - loss: 349.1084 - rmse_: 18.6420 - val_loss: 317.3066 - val_rmse_: 17.7641\n",
      "Epoch 488/1000\n",
      "112247/112247 [==============================] - 3s - loss: 347.7258 - rmse_: 18.6031 - val_loss: 317.3304 - val_rmse_: 17.7636\n",
      "Epoch 489/1000\n",
      "112247/112247 [==============================] - 3s - loss: 347.8986 - rmse_: 18.6059 - val_loss: 317.1591 - val_rmse_: 17.7603\n",
      "Epoch 490/1000\n",
      "112247/112247 [==============================] - 3s - loss: 345.9216 - rmse_: 18.5614 - val_loss: 317.2142 - val_rmse_: 17.7598\n",
      "Epoch 491/1000\n",
      "112247/112247 [==============================] - 3s - loss: 343.6941 - rmse_: 18.5043 - val_loss: 316.7701 - val_rmse_: 17.7511\n",
      "Epoch 492/1000\n",
      "112247/112247 [==============================] - 3s - loss: 349.1413 - rmse_: 18.6483 - val_loss: 316.8441 - val_rmse_: 17.7513\n",
      "Epoch 493/1000\n",
      "112247/112247 [==============================] - 3s - loss: 348.0140 - rmse_: 18.6210 - val_loss: 316.7660 - val_rmse_: 17.7486\n",
      "Epoch 494/1000\n",
      "112247/112247 [==============================] - 2s - loss: 347.7491 - rmse_: 18.6128 - val_loss: 316.8705 - val_rmse_: 17.7497\n",
      "Epoch 495/1000\n",
      "112247/112247 [==============================] - 3s - loss: 345.5501 - rmse_: 18.5517 - val_loss: 316.6772 - val_rmse_: 17.7457\n",
      "Epoch 496/1000\n",
      "112247/112247 [==============================] - 3s - loss: 346.7086 - rmse_: 18.5859 - val_loss: 316.4547 - val_rmse_: 17.7392\n",
      "Epoch 497/1000\n",
      "112247/112247 [==============================] - 3s - loss: 347.8193 - rmse_: 18.6108 - val_loss: 316.5647 - val_rmse_: 17.7394\n",
      "Epoch 498/1000\n",
      "112247/112247 [==============================] - 3s - loss: 346.7282 - rmse_: 18.5814 - val_loss: 316.5056 - val_rmse_: 17.7397\n",
      "Epoch 499/1000\n",
      "112247/112247 [==============================] - 3s - loss: 348.1753 - rmse_: 18.6227 - val_loss: 316.7818 - val_rmse_: 17.7437\n",
      "Epoch 500/1000\n",
      "112247/112247 [==============================] - 3s - loss: 347.3333 - rmse_: 18.5992 - val_loss: 316.2518 - val_rmse_: 17.7324\n",
      "Epoch 501/1000\n",
      "112247/112247 [==============================] - 3s - loss: 348.1858 - rmse_: 18.6207 - val_loss: 316.3065 - val_rmse_: 17.7349\n",
      "Epoch 502/1000\n",
      "112247/112247 [==============================] - 3s - loss: 345.8750 - rmse_: 18.5553 - val_loss: 315.8407 - val_rmse_: 17.7209\n",
      "Epoch 503/1000\n",
      "112247/112247 [==============================] - 3s - loss: 346.8658 - rmse_: 18.5877 - val_loss: 316.0407 - val_rmse_: 17.7268\n",
      "Epoch 504/1000\n",
      "112247/112247 [==============================] - 3s - loss: 346.1543 - rmse_: 18.5613 - val_loss: 315.9366 - val_rmse_: 17.7237\n",
      "Epoch 505/1000\n",
      "112247/112247 [==============================] - 3s - loss: 344.5571 - rmse_: 18.5252 - val_loss: 315.4442 - val_rmse_: 17.7123\n",
      "Epoch 506/1000\n",
      "112247/112247 [==============================] - 3s - loss: 346.9152 - rmse_: 18.5824 - val_loss: 315.3258 - val_rmse_: 17.7085\n",
      "Epoch 507/1000\n",
      "112247/112247 [==============================] - 3s - loss: 345.3444 - rmse_: 18.5546 - val_loss: 315.3419 - val_rmse_: 17.7084\n",
      "Epoch 508/1000\n",
      "112247/112247 [==============================] - 3s - loss: 343.4696 - rmse_: 18.4975 - val_loss: 315.2992 - val_rmse_: 17.7052\n",
      "Epoch 509/1000\n",
      "112247/112247 [==============================] - 3s - loss: 344.8756 - rmse_: 18.5299 - val_loss: 315.2469 - val_rmse_: 17.7049\n",
      "Epoch 510/1000\n",
      "112247/112247 [==============================] - 3s - loss: 346.4480 - rmse_: 18.5709 - val_loss: 315.2102 - val_rmse_: 17.7032\n",
      "Epoch 511/1000\n",
      "112247/112247 [==============================] - 3s - loss: 346.2050 - rmse_: 18.5685 - val_loss: 314.9774 - val_rmse_: 17.6983\n",
      "Epoch 512/1000\n",
      "112247/112247 [==============================] - 3s - loss: 345.4632 - rmse_: 18.5543 - val_loss: 314.8835 - val_rmse_: 17.6955\n",
      "Epoch 513/1000\n",
      "112247/112247 [==============================] - 3s - loss: 344.5439 - rmse_: 18.5202 - val_loss: 315.3053 - val_rmse_: 17.7019\n",
      "Epoch 514/1000\n",
      "112247/112247 [==============================] - 3s - loss: 343.9668 - rmse_: 18.5056 - val_loss: 314.6773 - val_rmse_: 17.6909\n",
      "Epoch 515/1000\n",
      "112247/112247 [==============================] - 3s - loss: 344.8238 - rmse_: 18.5208 - val_loss: 314.7787 - val_rmse_: 17.6893\n",
      "Epoch 516/1000\n",
      "112247/112247 [==============================] - 3s - loss: 344.7736 - rmse_: 18.5288 - val_loss: 314.7023 - val_rmse_: 17.6898\n",
      "Epoch 517/1000\n",
      "112247/112247 [==============================] - 4s - loss: 342.2983 - rmse_: 18.4630 - val_loss: 314.4302 - val_rmse_: 17.6826\n",
      "Epoch 518/1000\n",
      "112247/112247 [==============================] - 3s - loss: 346.7051 - rmse_: 18.5756 - val_loss: 314.4393 - val_rmse_: 17.6821\n",
      "Epoch 519/1000\n",
      "112247/112247 [==============================] - 3s - loss: 344.3567 - rmse_: 18.5127 - val_loss: 314.6531 - val_rmse_: 17.6855\n",
      "Epoch 520/1000\n",
      "112247/112247 [==============================] - 3s - loss: 345.6401 - rmse_: 18.5496 - val_loss: 314.2605 - val_rmse_: 17.6794\n",
      "Epoch 521/1000\n",
      "112247/112247 [==============================] - 3s - loss: 344.7861 - rmse_: 18.5357 - val_loss: 314.3440 - val_rmse_: 17.6778\n",
      "Epoch 522/1000\n",
      "112247/112247 [==============================] - 3s - loss: 344.8515 - rmse_: 18.5277 - val_loss: 314.1232 - val_rmse_: 17.6731\n",
      "Epoch 523/1000\n",
      "112247/112247 [==============================] - 3s - loss: 345.7182 - rmse_: 18.5537 - val_loss: 313.7691 - val_rmse_: 17.6677\n",
      "Epoch 524/1000\n",
      "112247/112247 [==============================] - 3s - loss: 344.2750 - rmse_: 18.5172 - val_loss: 314.1987 - val_rmse_: 17.6733\n",
      "Epoch 525/1000\n",
      "112247/112247 [==============================] - 3s - loss: 346.0855 - rmse_: 18.5591 - val_loss: 313.9199 - val_rmse_: 17.6680\n",
      "Epoch 526/1000\n",
      "112247/112247 [==============================] - 3s - loss: 344.8560 - rmse_: 18.5321 - val_loss: 313.8987 - val_rmse_: 17.6670\n",
      "Epoch 527/1000\n",
      "112247/112247 [==============================] - 4s - loss: 344.6025 - rmse_: 18.5241 - val_loss: 313.3813 - val_rmse_: 17.6568\n",
      "Epoch 528/1000\n",
      "112247/112247 [==============================] - 3s - loss: 345.2748 - rmse_: 18.5476 - val_loss: 313.8507 - val_rmse_: 17.6657\n",
      "Epoch 529/1000\n",
      "112247/112247 [==============================] - 3s - loss: 344.4405 - rmse_: 18.5206 - val_loss: 313.7135 - val_rmse_: 17.6607\n",
      "Epoch 530/1000\n",
      "112247/112247 [==============================] - 3s - loss: 343.8674 - rmse_: 18.5035 - val_loss: 313.4890 - val_rmse_: 17.6554\n",
      "Epoch 531/1000\n",
      "112247/112247 [==============================] - 3s - loss: 345.8344 - rmse_: 18.5658 - val_loss: 314.1183 - val_rmse_: 17.6670\n",
      "Epoch 532/1000\n",
      "112247/112247 [==============================] - 3s - loss: 346.8405 - rmse_: 18.5834 - val_loss: 313.3001 - val_rmse_: 17.6525\n",
      "Epoch 533/1000\n",
      "112247/112247 [==============================] - 3s - loss: 346.0961 - rmse_: 18.5491 - val_loss: 313.6896 - val_rmse_: 17.6603\n",
      "Epoch 534/1000\n",
      "112247/112247 [==============================] - 3s - loss: 344.0373 - rmse_: 18.5019 - val_loss: 313.1585 - val_rmse_: 17.6483\n",
      "Epoch 535/1000\n",
      "112247/112247 [==============================] - 3s - loss: 339.7762 - rmse_: 18.3925 - val_loss: 312.7269 - val_rmse_: 17.6384\n",
      "Epoch 536/1000\n",
      "112247/112247 [==============================] - 3s - loss: 343.2060 - rmse_: 18.4824 - val_loss: 313.4414 - val_rmse_: 17.6513\n",
      "Epoch 537/1000\n",
      "112247/112247 [==============================] - 3s - loss: 342.6492 - rmse_: 18.4674 - val_loss: 312.9211 - val_rmse_: 17.6397\n",
      "Epoch 538/1000\n",
      "112247/112247 [==============================] - 3s - loss: 341.3700 - rmse_: 18.4417 - val_loss: 313.2046 - val_rmse_: 17.6442\n",
      "Epoch 539/1000\n",
      "112247/112247 [==============================] - 3s - loss: 343.5330 - rmse_: 18.4910 - val_loss: 313.1417 - val_rmse_: 17.6430\n",
      "Epoch 540/1000\n",
      "112247/112247 [==============================] - 3s - loss: 343.9663 - rmse_: 18.5049 - val_loss: 313.0294 - val_rmse_: 17.6411\n",
      "Epoch 541/1000\n",
      "112247/112247 [==============================] - 3s - loss: 344.0629 - rmse_: 18.5115 - val_loss: 312.8488 - val_rmse_: 17.6385\n",
      "Epoch 542/1000\n",
      "112247/112247 [==============================] - 3s - loss: 342.5116 - rmse_: 18.4728 - val_loss: 312.7004 - val_rmse_: 17.6343\n",
      "Epoch 543/1000\n",
      "112247/112247 [==============================] - 3s - loss: 343.3890 - rmse_: 18.4916 - val_loss: 312.6038 - val_rmse_: 17.6321\n",
      "Epoch 544/1000\n",
      "112247/112247 [==============================] - 3s - loss: 341.4267 - rmse_: 18.4405 - val_loss: 312.5657 - val_rmse_: 17.6312\n",
      "Epoch 545/1000\n",
      "112247/112247 [==============================] - 3s - loss: 342.4338 - rmse_: 18.4753 - val_loss: 312.6230 - val_rmse_: 17.6324\n",
      "Epoch 546/1000\n",
      "112247/112247 [==============================] - 3s - loss: 343.2121 - rmse_: 18.4846 - val_loss: 312.6356 - val_rmse_: 17.6321\n",
      "Epoch 547/1000\n",
      "112247/112247 [==============================] - 3s - loss: 341.8640 - rmse_: 18.4509 - val_loss: 312.6188 - val_rmse_: 17.6320\n",
      "Epoch 548/1000\n",
      "112247/112247 [==============================] - 3s - loss: 342.4177 - rmse_: 18.4648 - val_loss: 312.5322 - val_rmse_: 17.6299\n",
      "Epoch 549/1000\n",
      "112247/112247 [==============================] - 3s - loss: 344.8038 - rmse_: 18.5282 - val_loss: 312.5535 - val_rmse_: 17.6300\n",
      "Epoch 550/1000\n",
      "112247/112247 [==============================] - 3s - loss: 344.8903 - rmse_: 18.5341 - val_loss: 312.4802 - val_rmse_: 17.6286\n",
      "Epoch 551/1000\n",
      "112247/112247 [==============================] - 3s - loss: 341.1326 - rmse_: 18.4259 - val_loss: 312.4062 - val_rmse_: 17.6265\n",
      "Epoch 552/1000\n",
      "112247/112247 [==============================] - 3s - loss: 340.6731 - rmse_: 18.4110 - val_loss: 312.4118 - val_rmse_: 17.6266\n",
      "Epoch 553/1000\n",
      "112247/112247 [==============================] - 3s - loss: 342.6267 - rmse_: 18.4717 - val_loss: 312.3855 - val_rmse_: 17.6261\n",
      "Epoch 554/1000\n",
      "112247/112247 [==============================] - 3s - loss: 343.2482 - rmse_: 18.4860 - val_loss: 312.3018 - val_rmse_: 17.6242\n",
      "Epoch 555/1000\n",
      "112247/112247 [==============================] - 3s - loss: 344.0372 - rmse_: 18.5077 - val_loss: 312.2764 - val_rmse_: 17.6230\n",
      "Epoch 556/1000\n",
      "112247/112247 [==============================] - 3s - loss: 344.6656 - rmse_: 18.5217 - val_loss: 312.2899 - val_rmse_: 17.6237\n",
      "Epoch 557/1000\n",
      "112247/112247 [==============================] - 3s - loss: 343.3516 - rmse_: 18.4883 - val_loss: 312.2580 - val_rmse_: 17.6227\n",
      "Epoch 558/1000\n",
      "112247/112247 [==============================] - 3s - loss: 343.0825 - rmse_: 18.4751 - val_loss: 312.3987 - val_rmse_: 17.6250\n",
      "Epoch 559/1000\n",
      "112247/112247 [==============================] - 3s - loss: 341.6351 - rmse_: 18.4428 - val_loss: 312.2338 - val_rmse_: 17.6216\n",
      "Epoch 560/1000\n",
      "112247/112247 [==============================] - 3s - loss: 344.0565 - rmse_: 18.5075 - val_loss: 312.3931 - val_rmse_: 17.6245\n",
      "Epoch 561/1000\n",
      "112247/112247 [==============================] - 3s - loss: 343.9094 - rmse_: 18.5125 - val_loss: 312.2600 - val_rmse_: 17.6218\n",
      "Epoch 562/1000\n",
      "112247/112247 [==============================] - 3s - loss: 341.7463 - rmse_: 18.4473 - val_loss: 312.2161 - val_rmse_: 17.6209\n",
      "Epoch 563/1000\n",
      "112247/112247 [==============================] - 3s - loss: 341.0911 - rmse_: 18.4335 - val_loss: 312.1990 - val_rmse_: 17.6202\n",
      "Epoch 564/1000\n",
      "112247/112247 [==============================] - 3s - loss: 342.6817 - rmse_: 18.4747 - val_loss: 312.2406 - val_rmse_: 17.6209\n",
      "Epoch 565/1000\n",
      "112247/112247 [==============================] - 3s - loss: 342.6361 - rmse_: 18.4726 - val_loss: 312.1837 - val_rmse_: 17.6204\n",
      "Epoch 566/1000\n",
      "112247/112247 [==============================] - 3s - loss: 344.1764 - rmse_: 18.5131 - val_loss: 312.0602 - val_rmse_: 17.6175\n",
      "Epoch 567/1000\n",
      "112247/112247 [==============================] - 3s - loss: 342.0471 - rmse_: 18.4491 - val_loss: 312.0462 - val_rmse_: 17.6166\n",
      "Epoch 568/1000\n",
      "112247/112247 [==============================] - 3s - loss: 343.6268 - rmse_: 18.4990 - val_loss: 312.0893 - val_rmse_: 17.6170\n",
      "Epoch 569/1000\n",
      "112247/112247 [==============================] - 3s - loss: 340.9229 - rmse_: 18.4200 - val_loss: 312.0339 - val_rmse_: 17.6163\n",
      "Epoch 570/1000\n",
      "112247/112247 [==============================] - 3s - loss: 342.3867 - rmse_: 18.4575 - val_loss: 312.1904 - val_rmse_: 17.6191\n",
      "Epoch 571/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112247/112247 [==============================] - 3s - loss: 341.9611 - rmse_: 18.4562 - val_loss: 312.0126 - val_rmse_: 17.6155\n",
      "Epoch 572/1000\n",
      "112247/112247 [==============================] - 3s - loss: 341.6863 - rmse_: 18.4489 - val_loss: 312.0411 - val_rmse_: 17.6157\n",
      "Epoch 573/1000\n",
      "112247/112247 [==============================] - 3s - loss: 342.5715 - rmse_: 18.4655 - val_loss: 312.1286 - val_rmse_: 17.6175\n",
      "Epoch 574/1000\n",
      "112247/112247 [==============================] - 3s - loss: 342.2327 - rmse_: 18.4621 - val_loss: 312.0691 - val_rmse_: 17.6164\n",
      "Epoch 575/1000\n",
      "112247/112247 [==============================] - 3s - loss: 342.3394 - rmse_: 18.4582 - val_loss: 312.0194 - val_rmse_: 17.6149\n",
      "Epoch 576/1000\n",
      "112247/112247 [==============================] - 3s - loss: 342.6522 - rmse_: 18.4698 - val_loss: 312.0005 - val_rmse_: 17.6144\n",
      "Epoch 577/1000\n",
      "112247/112247 [==============================] - 3s - loss: 344.4672 - rmse_: 18.5166 - val_loss: 312.0380 - val_rmse_: 17.6148\n",
      "Epoch 578/1000\n",
      "112247/112247 [==============================] - 3s - loss: 341.3257 - rmse_: 18.4337 - val_loss: 311.9348 - val_rmse_: 17.6132\n",
      "Epoch 579/1000\n",
      "112247/112247 [==============================] - 3s - loss: 342.5663 - rmse_: 18.4724 - val_loss: 311.9581 - val_rmse_: 17.6135\n",
      "Epoch 580/1000\n",
      "112247/112247 [==============================] - 3s - loss: 342.9896 - rmse_: 18.4837 - val_loss: 311.9580 - val_rmse_: 17.6133\n",
      "Epoch 581/1000\n",
      "112247/112247 [==============================] - 3s - loss: 343.4806 - rmse_: 18.4946 - val_loss: 311.9833 - val_rmse_: 17.6139\n",
      "Epoch 582/1000\n",
      "112247/112247 [==============================] - 3s - loss: 341.7603 - rmse_: 18.4458 - val_loss: 311.8827 - val_rmse_: 17.6119\n",
      "Epoch 583/1000\n",
      "112247/112247 [==============================] - 3s - loss: 339.3554 - rmse_: 18.3843 - val_loss: 311.8409 - val_rmse_: 17.6108\n",
      "Epoch 584/1000\n",
      "112247/112247 [==============================] - 3s - loss: 342.1701 - rmse_: 18.4557 - val_loss: 311.8735 - val_rmse_: 17.6110\n",
      "Epoch 585/1000\n",
      "112247/112247 [==============================] - 3s - loss: 342.9886 - rmse_: 18.4745 - val_loss: 311.9080 - val_rmse_: 17.6115\n",
      "Epoch 586/1000\n",
      "112247/112247 [==============================] - 3s - loss: 340.4455 - rmse_: 18.4182 - val_loss: 311.7755 - val_rmse_: 17.6087\n",
      "Epoch 587/1000\n",
      "112247/112247 [==============================] - 3s - loss: 340.3081 - rmse_: 18.4059 - val_loss: 311.6932 - val_rmse_: 17.6065\n",
      "Epoch 588/1000\n",
      "112247/112247 [==============================] - 3s - loss: 343.6104 - rmse_: 18.4925 - val_loss: 311.7339 - val_rmse_: 17.6077\n",
      "Epoch 589/1000\n",
      "112247/112247 [==============================] - 3s - loss: 341.2191 - rmse_: 18.4334 - val_loss: 311.7524 - val_rmse_: 17.6070\n",
      "Epoch 590/1000\n",
      "112247/112247 [==============================] - 3s - loss: 342.3461 - rmse_: 18.4590 - val_loss: 311.8676 - val_rmse_: 17.6096\n",
      "Epoch 591/1000\n",
      "112247/112247 [==============================] - 3s - loss: 340.2999 - rmse_: 18.4042 - val_loss: 311.7363 - val_rmse_: 17.6075\n",
      "Epoch 592/1000\n",
      "112247/112247 [==============================] - 3s - loss: 343.2261 - rmse_: 18.4909 - val_loss: 311.7736 - val_rmse_: 17.6081\n",
      "Epoch 593/1000\n",
      "112247/112247 [==============================] - 3s - loss: 342.0831 - rmse_: 18.4562 - val_loss: 311.6223 - val_rmse_: 17.6048\n",
      "Epoch 594/1000\n",
      "112247/112247 [==============================] - 3s - loss: 343.1956 - rmse_: 18.4842 - val_loss: 311.6748 - val_rmse_: 17.6059\n",
      "Epoch 595/1000\n",
      "112247/112247 [==============================] - 3s - loss: 340.7890 - rmse_: 18.4204 - val_loss: 311.5972 - val_rmse_: 17.6041\n",
      "Epoch 596/1000\n",
      "112247/112247 [==============================] - 3s - loss: 342.2583 - rmse_: 18.4608 - val_loss: 311.6338 - val_rmse_: 17.6048\n",
      "Epoch 597/1000\n",
      "112247/112247 [==============================] - 2s - loss: 339.3925 - rmse_: 18.3846 - val_loss: 311.6385 - val_rmse_: 17.6049\n",
      "Epoch 598/1000\n",
      "112247/112247 [==============================] - 3s - loss: 343.0604 - rmse_: 18.4850 - val_loss: 311.7899 - val_rmse_: 17.6078\n",
      "Epoch 599/1000\n",
      "112247/112247 [==============================] - 3s - loss: 339.7922 - rmse_: 18.3982 - val_loss: 311.5524 - val_rmse_: 17.6033\n",
      "Epoch 600/1000\n",
      "112247/112247 [==============================] - 3s - loss: 341.7332 - rmse_: 18.4425 - val_loss: 311.5459 - val_rmse_: 17.6029\n",
      "Epoch 601/1000\n",
      "112247/112247 [==============================] - 3s - loss: 341.9437 - rmse_: 18.4544 - val_loss: 311.5857 - val_rmse_: 17.6035\n",
      "Epoch 602/1000\n",
      "112247/112247 [==============================] - 3s - loss: 342.0680 - rmse_: 18.4635 - val_loss: 311.5167 - val_rmse_: 17.6017\n",
      "Epoch 603/1000\n",
      "112247/112247 [==============================] - 3s - loss: 341.8137 - rmse_: 18.4518 - val_loss: 311.5605 - val_rmse_: 17.6026\n",
      "Epoch 604/1000\n",
      "112247/112247 [==============================] - 3s - loss: 340.6992 - rmse_: 18.4168 - val_loss: 311.5360 - val_rmse_: 17.6018\n",
      "Epoch 605/1000\n",
      "112247/112247 [==============================] - 3s - loss: 343.0715 - rmse_: 18.4809 - val_loss: 311.5509 - val_rmse_: 17.6024\n",
      "Epoch 606/1000\n",
      "112247/112247 [==============================] - 3s - loss: 340.4909 - rmse_: 18.4154 - val_loss: 311.4796 - val_rmse_: 17.6005\n",
      "Epoch 607/1000\n",
      "112247/112247 [==============================] - 3s - loss: 341.3519 - rmse_: 18.4379 - val_loss: 311.4386 - val_rmse_: 17.5997\n",
      "Epoch 608/1000\n",
      "112247/112247 [==============================] - 3s - loss: 339.9178 - rmse_: 18.3911 - val_loss: 311.4400 - val_rmse_: 17.5993\n",
      "Epoch 609/1000\n",
      "112247/112247 [==============================] - 3s - loss: 341.2300 - rmse_: 18.4251 - val_loss: 311.3183 - val_rmse_: 17.5966\n",
      "Epoch 610/1000\n",
      "112247/112247 [==============================] - 3s - loss: 341.3216 - rmse_: 18.4332 - val_loss: 311.3238 - val_rmse_: 17.5970\n",
      "Epoch 611/1000\n",
      "112247/112247 [==============================] - 3s - loss: 342.9563 - rmse_: 18.4796 - val_loss: 311.3695 - val_rmse_: 17.5976\n",
      "Epoch 612/1000\n",
      "112247/112247 [==============================] - 3s - loss: 339.6615 - rmse_: 18.3863 - val_loss: 311.3397 - val_rmse_: 17.5969\n",
      "Epoch 613/1000\n",
      "112247/112247 [==============================] - 3s - loss: 340.9355 - rmse_: 18.4203 - val_loss: 311.3508 - val_rmse_: 17.5971\n",
      "Epoch 614/1000\n",
      "112247/112247 [==============================] - 3s - loss: 341.3305 - rmse_: 18.4357 - val_loss: 311.3517 - val_rmse_: 17.5973\n",
      "Epoch 615/1000\n",
      "112247/112247 [==============================] - 3s - loss: 340.6724 - rmse_: 18.4212 - val_loss: 311.2324 - val_rmse_: 17.5943\n",
      "Epoch 616/1000\n",
      "112247/112247 [==============================] - 3s - loss: 341.2907 - rmse_: 18.4315 - val_loss: 311.2822 - val_rmse_: 17.5953\n",
      "Epoch 617/1000\n",
      "112247/112247 [==============================] - 3s - loss: 344.1335 - rmse_: 18.5071 - val_loss: 311.3092 - val_rmse_: 17.5952\n",
      "Epoch 618/1000\n",
      "112247/112247 [==============================] - 3s - loss: 342.2592 - rmse_: 18.4595 - val_loss: 311.3486 - val_rmse_: 17.5963\n",
      "Epoch 619/1000\n",
      "112247/112247 [==============================] - 3s - loss: 341.1715 - rmse_: 18.4254 - val_loss: 311.3033 - val_rmse_: 17.5953\n",
      "Epoch 620/1000\n",
      "112247/112247 [==============================] - 3s - loss: 341.7154 - rmse_: 18.4532 - val_loss: 311.3220 - val_rmse_: 17.5951\n",
      "Epoch 621/1000\n",
      "112247/112247 [==============================] - 3s - loss: 341.4736 - rmse_: 18.4353 - val_loss: 311.2350 - val_rmse_: 17.5934\n",
      "Epoch 622/1000\n",
      "112247/112247 [==============================] - 3s - loss: 340.1495 - rmse_: 18.4009 - val_loss: 311.2289 - val_rmse_: 17.5933\n",
      "Epoch 623/1000\n",
      "112247/112247 [==============================] - 3s - loss: 340.4986 - rmse_: 18.4147 - val_loss: 311.1964 - val_rmse_: 17.5927\n",
      "Epoch 624/1000\n",
      "112247/112247 [==============================] - 3s - loss: 341.5369 - rmse_: 18.4409 - val_loss: 311.1869 - val_rmse_: 17.5924\n",
      "Epoch 625/1000\n",
      "112247/112247 [==============================] - 3s - loss: 339.6551 - rmse_: 18.3906 - val_loss: 311.2029 - val_rmse_: 17.5927\n",
      "Epoch 626/1000\n",
      "112247/112247 [==============================] - 3s - loss: 342.4775 - rmse_: 18.4664 - val_loss: 311.2315 - val_rmse_: 17.5933\n",
      "Epoch 627/1000\n",
      "112247/112247 [==============================] - 3s - loss: 340.7490 - rmse_: 18.4279 - val_loss: 311.2145 - val_rmse_: 17.5930\n",
      "Epoch 628/1000\n",
      "112247/112247 [==============================] - 3s - loss: 340.2462 - rmse_: 18.4037 - val_loss: 311.1821 - val_rmse_: 17.5924\n",
      "Epoch 629/1000\n",
      "112247/112247 [==============================] - 3s - loss: 343.2937 - rmse_: 18.4845 - val_loss: 311.2059 - val_rmse_: 17.5927\n",
      "Epoch 630/1000\n",
      "112247/112247 [==============================] - 3s - loss: 342.3468 - rmse_: 18.4603 - val_loss: 311.2143 - val_rmse_: 17.5929\n",
      "Epoch 631/1000\n",
      "112247/112247 [==============================] - 3s - loss: 340.9133 - rmse_: 18.4210 - val_loss: 311.1989 - val_rmse_: 17.5926\n",
      "Epoch 632/1000\n",
      "112247/112247 [==============================] - 3s - loss: 344.1040 - rmse_: 18.5033 - val_loss: 311.2272 - val_rmse_: 17.5933\n",
      "Epoch 633/1000\n",
      "112247/112247 [==============================] - 3s - loss: 342.1859 - rmse_: 18.4527 - val_loss: 311.2448 - val_rmse_: 17.5936\n",
      "Epoch 634/1000\n",
      "112247/112247 [==============================] - 3s - loss: 339.3846 - rmse_: 18.3835 - val_loss: 311.2197 - val_rmse_: 17.5930\n",
      "Epoch 635/1000\n",
      "112247/112247 [==============================] - 3s - loss: 342.0577 - rmse_: 18.4500 - val_loss: 311.2178 - val_rmse_: 17.5930\n",
      "Epoch 636/1000\n",
      "112247/112247 [==============================] - 3s - loss: 343.3673 - rmse_: 18.4950 - val_loss: 311.2139 - val_rmse_: 17.5929\n",
      "Epoch 637/1000\n",
      "112247/112247 [==============================] - 3s - loss: 342.5555 - rmse_: 18.4691 - val_loss: 311.2157 - val_rmse_: 17.5930\n",
      "Epoch 638/1000\n",
      "112247/112247 [==============================] - 3s - loss: 344.2927 - rmse_: 18.5182 - val_loss: 311.2096 - val_rmse_: 17.5928\n",
      "Epoch 639/1000\n",
      "112247/112247 [==============================] - 3s - loss: 340.1121 - rmse_: 18.4044 - val_loss: 311.2098 - val_rmse_: 17.5928\n",
      "Epoch 640/1000\n",
      "112247/112247 [==============================] - 3s - loss: 341.7610 - rmse_: 18.4424 - val_loss: 311.2107 - val_rmse_: 17.5928\n",
      "Epoch 641/1000\n",
      "112247/112247 [==============================] - 3s - loss: 340.9966 - rmse_: 18.4283 - val_loss: 311.2099 - val_rmse_: 17.5928\n"
     ]
    }
   ],
   "source": [
    "batch_size = 500\n",
    "epochs = 1000\n",
    "history = model.fit(oversampled_X_regression, oversampled_y_regression,\n",
    "                    validation_split=0.33,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    callbacks=[reduce_lr, earlystopper, modelchecker])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0b2c5659b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling w/ SMOTE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model_2(learn_rate=0.001, decay=0.01 ,init='normal', drop_out=0.2, loss='mse'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=40, kernel_initializer=init, input_shape=(smote_x.shape[1],)))\n",
    "    model.add(Activation(activation='relu'))\n",
    "    model.add(Dropout(drop_out))\n",
    "    model.add(Dense(units=20, kernel_initializer=init))\n",
    "    model.add(Activation(activation='relu'))\n",
    "    model.add(Dropout(drop_out))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss=loss, \n",
    "              optimizer=Adam(lr=learn_rate, decay=decay), \n",
    "              metrics=[rmse_])\n",
    "    return model\n",
    "\n",
    "model2 = create_model_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 40)                2040      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 20)                820       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 2,881\n",
      "Trainable params: 2,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                  factor=0.2, \n",
    "                  patience=5,   \n",
    "                  epsilon=0.0001,  \n",
    "                  min_lr=0)\n",
    "modelchecker = ModelCheckpoint('mlp_model_SMOTE.h5', save_best_only=True)\n",
    "\n",
    "earlystopper = EarlyStopping(patience=12, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 320028 samples, validate on 2280 samples\n",
      "Epoch 1/100\n",
      "320028/320028 [==============================] - 3s - loss: 322.8524 - rmse_: 17.8912 - val_loss: 604.9241 - val_rmse_: 24.5142\n",
      "Epoch 2/100\n",
      "320028/320028 [==============================] - 3s - loss: 323.0171 - rmse_: 17.8998 - val_loss: 604.7950 - val_rmse_: 24.5116\n",
      "Epoch 3/100\n",
      "320028/320028 [==============================] - 3s - loss: 322.8675 - rmse_: 17.8885 - val_loss: 604.5996 - val_rmse_: 24.5077\n",
      "Epoch 4/100\n",
      "320028/320028 [==============================] - 3s - loss: 322.8083 - rmse_: 17.8915 - val_loss: 604.5450 - val_rmse_: 24.5066\n",
      "Epoch 5/100\n",
      "320028/320028 [==============================] - 3s - loss: 322.8303 - rmse_: 17.8872 - val_loss: 604.4194 - val_rmse_: 24.5040\n",
      "Epoch 6/100\n",
      "320028/320028 [==============================] - 3s - loss: 322.8706 - rmse_: 17.8925 - val_loss: 604.4239 - val_rmse_: 24.5041\n",
      "Epoch 7/100\n",
      "320028/320028 [==============================] - 3s - loss: 322.7970 - rmse_: 17.8906 - val_loss: 604.3729 - val_rmse_: 24.5031\n",
      "Epoch 8/100\n",
      "320028/320028 [==============================] - 3s - loss: 322.6831 - rmse_: 17.8880 - val_loss: 604.3645 - val_rmse_: 24.5029\n",
      "Epoch 9/100\n",
      "320028/320028 [==============================] - 3s - loss: 322.7230 - rmse_: 17.8881 - val_loss: 604.2543 - val_rmse_: 24.5007\n",
      "Epoch 10/100\n",
      "320028/320028 [==============================] - 2s - loss: 322.7156 - rmse_: 17.8837 - val_loss: 604.2824 - val_rmse_: 24.5012\n",
      "Epoch 11/100\n",
      "320028/320028 [==============================] - 2s - loss: 322.7303 - rmse_: 17.8842 - val_loss: 604.2824 - val_rmse_: 24.5012\n",
      "Epoch 12/100\n",
      "320028/320028 [==============================] - 2s - loss: 322.8464 - rmse_: 17.8912 - val_loss: 604.3014 - val_rmse_: 24.5016\n",
      "Epoch 13/100\n",
      "320028/320028 [==============================] - 2s - loss: 322.7253 - rmse_: 17.8876 - val_loss: 604.3087 - val_rmse_: 24.5018\n",
      "Epoch 14/100\n",
      "320028/320028 [==============================] - 2s - loss: 322.8991 - rmse_: 17.8905 - val_loss: 604.3243 - val_rmse_: 24.5021\n",
      "Epoch 15/100\n",
      "320028/320028 [==============================] - 3s - loss: 322.7359 - rmse_: 17.8837 - val_loss: 604.2619 - val_rmse_: 24.5008\n",
      "Epoch 16/100\n",
      "320028/320028 [==============================] - 2s - loss: 322.8555 - rmse_: 17.8889 - val_loss: 604.2572 - val_rmse_: 24.5007\n",
      "Epoch 17/100\n",
      "320028/320028 [==============================] - 2s - loss: 322.6896 - rmse_: 17.8813 - val_loss: 604.2351 - val_rmse_: 24.5003\n",
      "Epoch 18/100\n",
      "320028/320028 [==============================] - 2s - loss: 322.8250 - rmse_: 17.8864 - val_loss: 604.2405 - val_rmse_: 24.5004\n",
      "Epoch 19/100\n",
      "320028/320028 [==============================] - 2s - loss: 322.6793 - rmse_: 17.8752 - val_loss: 604.2280 - val_rmse_: 24.5001\n",
      "Epoch 20/100\n",
      "320028/320028 [==============================] - 2s - loss: 322.6753 - rmse_: 17.8786 - val_loss: 604.2183 - val_rmse_: 24.4999\n",
      "Epoch 21/100\n",
      "320028/320028 [==============================] - 2s - loss: 322.5819 - rmse_: 17.8812 - val_loss: 604.2151 - val_rmse_: 24.4999\n",
      "Epoch 22/100\n",
      "320028/320028 [==============================] - 2s - loss: 322.7124 - rmse_: 17.8808 - val_loss: 604.2176 - val_rmse_: 24.4999\n",
      "Epoch 23/100\n",
      "320028/320028 [==============================] - 2s - loss: 322.6928 - rmse_: 17.8869 - val_loss: 604.2162 - val_rmse_: 24.4999\n",
      "Epoch 24/100\n",
      "320028/320028 [==============================] - 3s - loss: 322.7684 - rmse_: 17.8901 - val_loss: 604.2268 - val_rmse_: 24.5001\n",
      "Epoch 25/100\n",
      "320028/320028 [==============================] - 2s - loss: 322.6513 - rmse_: 17.8832 - val_loss: 604.2227 - val_rmse_: 24.5000\n",
      "Epoch 26/100\n",
      "320028/320028 [==============================] - 2s - loss: 322.8107 - rmse_: 17.8867 - val_loss: 604.2211 - val_rmse_: 24.5000\n",
      "Epoch 27/100\n",
      "320028/320028 [==============================] - 3s - loss: 322.6500 - rmse_: 17.8774 - val_loss: 604.2159 - val_rmse_: 24.4999\n",
      "Epoch 28/100\n",
      "320028/320028 [==============================] - 3s - loss: 322.7374 - rmse_: 17.8876 - val_loss: 604.2160 - val_rmse_: 24.4999\n",
      "Epoch 29/100\n",
      "320028/320028 [==============================] - 2s - loss: 322.8093 - rmse_: 17.8932 - val_loss: 604.2157 - val_rmse_: 24.4999\n",
      "Epoch 30/100\n",
      "320028/320028 [==============================] - 3s - loss: 322.5847 - rmse_: 17.8825 - val_loss: 604.2139 - val_rmse_: 24.4998\n",
      "Epoch 31/100\n",
      "320028/320028 [==============================] - 2s - loss: 322.7700 - rmse_: 17.8911 - val_loss: 604.2135 - val_rmse_: 24.4998\n",
      "Epoch 32/100\n",
      "320028/320028 [==============================] - 2s - loss: 322.6714 - rmse_: 17.8890 - val_loss: 604.2131 - val_rmse_: 24.4998\n",
      "Epoch 33/100\n",
      "320028/320028 [==============================] - 2s - loss: 322.6065 - rmse_: 17.8807 - val_loss: 604.2118 - val_rmse_: 24.4998\n",
      "Epoch 34/100\n",
      "320028/320028 [==============================] - 3s - loss: 322.7164 - rmse_: 17.8829 - val_loss: 604.2126 - val_rmse_: 24.4998\n",
      "Epoch 35/100\n",
      "320028/320028 [==============================] - 2s - loss: 322.8435 - rmse_: 17.8879 - val_loss: 604.2124 - val_rmse_: 24.4998\n",
      "Epoch 36/100\n",
      "320028/320028 [==============================] - 2s - loss: 322.6335 - rmse_: 17.8846 - val_loss: 604.2125 - val_rmse_: 24.4998\n",
      "Epoch 37/100\n",
      "320028/320028 [==============================] - 3s - loss: 322.7050 - rmse_: 17.8888 - val_loss: 604.2109 - val_rmse_: 24.4998\n",
      "Epoch 38/100\n",
      "320028/320028 [==============================] - 2s - loss: 322.7173 - rmse_: 17.8913 - val_loss: 604.2116 - val_rmse_: 24.4998\n",
      "Epoch 39/100\n",
      "320028/320028 [==============================] - 2s - loss: 322.8623 - rmse_: 17.8883 - val_loss: 604.2111 - val_rmse_: 24.4998\n",
      "Epoch 40/100\n",
      "320028/320028 [==============================] - 2s - loss: 322.7042 - rmse_: 17.8844 - val_loss: 604.2098 - val_rmse_: 24.4998\n",
      "Epoch 41/100\n",
      "320028/320028 [==============================] - 2s - loss: 322.8487 - rmse_: 17.8998 - val_loss: 604.2110 - val_rmse_: 24.4998\n",
      "Epoch 42/100\n",
      "320028/320028 [==============================] - 2s - loss: 322.8019 - rmse_: 17.8893 - val_loss: 604.2120 - val_rmse_: 24.4998\n",
      "Epoch 43/100\n",
      "320028/320028 [==============================] - 2s - loss: 322.6706 - rmse_: 17.8855 - val_loss: 604.2116 - val_rmse_: 24.4998\n",
      "Epoch 44/100\n",
      "320028/320028 [==============================] - 2s - loss: 322.6726 - rmse_: 17.8852 - val_loss: 604.2116 - val_rmse_: 24.4998\n",
      "Epoch 45/100\n",
      "320028/320028 [==============================] - 2s - loss: 322.7253 - rmse_: 17.8830 - val_loss: 604.2118 - val_rmse_: 24.4998\n",
      "Epoch 46/100\n",
      "320028/320028 [==============================] - 2s - loss: 322.6909 - rmse_: 17.8888 - val_loss: 604.2125 - val_rmse_: 24.4998\n",
      "Epoch 47/100\n",
      "320028/320028 [==============================] - 2s - loss: 322.7319 - rmse_: 17.8879 - val_loss: 604.2125 - val_rmse_: 24.4998\n",
      "Epoch 48/100\n",
      "320028/320028 [==============================] - 2s - loss: 322.7954 - rmse_: 17.8980 - val_loss: 604.2126 - val_rmse_: 24.4998\n",
      "Epoch 49/100\n",
      "320028/320028 [==============================] - 2s - loss: 322.7437 - rmse_: 17.8791 - val_loss: 604.2125 - val_rmse_: 24.4998\n",
      "Epoch 50/100\n",
      "320028/320028 [==============================] - 2s - loss: 322.7083 - rmse_: 17.8742 - val_loss: 604.2128 - val_rmse_: 24.4998\n",
      "Epoch 51/100\n",
      "320028/320028 [==============================] - 2s - loss: 322.8156 - rmse_: 17.8852 - val_loss: 604.2130 - val_rmse_: 24.4998\n",
      "Epoch 52/100\n",
      "320028/320028 [==============================] - 2s - loss: 322.7172 - rmse_: 17.8838 - val_loss: 604.2130 - val_rmse_: 24.4998\n",
      "Epoch 53/100\n",
      "320028/320028 [==============================] - 3s - loss: 322.9357 - rmse_: 17.8883 - val_loss: 604.2130 - val_rmse_: 24.4998\n"
     ]
    }
   ],
   "source": [
    "batch_size = 500\n",
    "epochs = 100\n",
    "history = model2.fit(smote_x, smote_y,\n",
    "                    validation_data=(dim_reduced_test_x[y_test>0], y_test[y_test>0]),\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    callbacks=[reduce_lr, earlystopper, modelchecker])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHmtJREFUeJzt3X14FOW9//H3d0MIVfSoGB7kGQ8WlGCgAbH8SkGsqLVQ\nWx8QK9RascVjrW0taG3VXlpr7dV6/BVp+dUHVDxCsVVOpRZEkUOvagwtjwaFclADaAIIQi0i2e/v\nj51NNiEPE9jNhtnP67py7cy998x873tnvrl3dnfG3B0REYmuWLYDEBGRzFKiFxGJOCV6EZGIU6IX\nEYk4JXoRkYhTohcRiTglehGRiFOiFxGJOCV6EZGIa5ftAABOPvlk79OnT7bDEBE5qqxcuXKHuxc2\nV69NJPo+ffpQVlaW7TBERI4qZvZWmHo6dSMiEnFK9CIiEadELyIScW3iHL3klo8//piKigr279+f\n7VAio0OHDvTo0YP8/PxshyJtkBK9tLqKigqOO+44+vTpg5llO5yjnruzc+dOKioq6Nu3b7bDkTZI\np26k1e3fv59OnTopyaeJmdGpUye9Q5JGhUr0ZnaCmS0wsw1mVm5mZ5vZSWa2xMw2Bo8nBnXNzB4w\ns01mtsbMhma2CXI0UpJPL/WnNCXsqZv/BJ5390vMrD1wDHArsNTdf2pmM4AZwHTgAqB/8HcWMCt4\nbNTBykqq/u+vIGZghsViYLHEdF5i2vJiEMuDmGF5eYnnAZL7t1liZzdLFMZiWLA+LJZYdzNqDpY6\nj40sZ5Z4qiZeq/1LCnubRm90pollQtRLxm/UxBUqIdSv0+Ik0nT9eMdjqd67t4XrzKajI4nG93/E\nvhV/yXYY0gY1m+jN7HhgFPBVAHc/ABwwswnA6KDaHGAZiUQ/AXjMEzejfSV4N9DN3bc3to2DlVXs\nmDnzCJohR5Pqmb/iwFuhfueRMYXDh1NVWprVGNKtetdO3rn+P7IdhrRBYUb0/YAq4BEzOxNYCdwI\ndEkmb3ffbmadg/rdgXdSlq8IyhpN9B0GncGA115LjFLjcXDHk9PxOB6PQ3V14jEex6urg4FvMKp1\nrx3hxuPBZMryyXU1NTJ1P+SxyRunJ7cZ/Hk8OV1/OyFHg6nLhF6k8Yrunuged+r0U3Pq12nhzePD\n3Gz+f2Mx2vfrl7IQLR80H84yqerH0NR2jhJ51dX0fvLJbIchrelT4c6Mh0n07YChwA3u/qqZ/SeJ\n0zSNaejwO+RwMbOpwFSAXr161Z52icUaXYlEQ6y8nLxjjsl2GOQdcwzuzve//33+9Kc/YWbcdttt\nXH755Wzfvp3LL7+cDz74gIMHDzJr1iw+/elPc80111BWVoaZ8bWvfY2bbrop282oEWvfnmMGDsx2\nGNIGhUn0FUCFu78azC8gkejfS56SMbNuQGVK/Z4py/cAttVfqbvPBmYDlJSUHEXjJkmnO/97Pa9v\n+yCt6zz9lOO5/QtnhKr7+9//nlWrVrF69Wp27NjBsGHDGDVqFE8++STjxo3jBz/4AdXV1Xz44Yes\nWrWKrVu3sm7dOgB2796d1rhFMqXZb924+7vAO2b2yaBoLPA6sBCYEpRNAZ4NphcCk4Nv34wA9jR1\nfl4km1asWMEVV1xBXl4eXbp04bOf/SyvvfYaw4YN45FHHuGOO+5g7dq1HHfccfTr14/Nmzdzww03\n8Pzzz3P88cdnO3yRUMJ+6+YGYG7wjZvNwNUk/knMN7NrgLeBS4O6i4ALgU3Ah0FdkQaFHXlnSmOf\nKYwaNYrly5fz3HPPcdVVV3HzzTczefJkVq9ezZ///GdmzpzJ/Pnzefjhh1s5YpGWC5Xo3X0VUNLA\nU2MbqOvA9UcYl0irGDVqFL/5zW+YMmUKu3btYvny5dx333289dZbdO/enWuvvZZ//vOf/O1vf+PC\nCy+kffv2fPnLX+bUU0/lq1/9arbDFwlFl0CQnHbxxRfz17/+lTPPPBMz42c/+xldu3Zlzpw53Hff\nfeTn59OxY0cee+wxtm7dytVXX008HgfgnnvuyXL0IuFYmK/DZVpJSYnrxiO5o7y8nIH6dkjaqV9z\nj5mtdPeGzrbUoWvdiIhEnBK9iEjEKdGLiEScEr2ISMQp0YuIRJwSvYhIxCnRS07q2LFjtkMQaTVK\n9CKB6urqbIcgkhFK9JLTli1bxpgxY5g0aRJFRUXZDkckI3QJBMmuP82Ad9emd51di+CCn4auXlpa\nyrp16+jbt2964xBpIzSil5w3fPhwJXmJNI3oJbtaMPLOlGOPPTbbIYhklEb0IiIRp0QvIhJxOnUj\nOWnfvn0AjB49mtGjR2c3GJEM04heRCTilOhFRCJOiV5EJOKU6EVEIk6JXkQk4pToRUQiToleRCTi\nQiV6M9tiZmvNbJWZlQVlJ5nZEjPbGDyeGJSbmT1gZpvMbI2ZDc1kA0SOlLsTj8ezHYZIxrRkRD/G\n3YvdvSSYnwEsdff+wNJgHuACoH/wNxWYla5gRdJly5YtDBw4kGnTpjF06FDy8vKYPn06n/rUpzj3\n3HMpLS1l9OjR9OvXj4ULFwKwfv16hg8fTnFxMYMHD2bjxo0APPHEEzXl1113na5rL23OkfwydgIw\nOpieAywDpgflj7m7A6+Y2Qlm1s3dtx9JoBJN95bey4ZdG9K6zgEnDWD68OnN1nvjjTd45JFHePDB\nBzEzRo8ezb333svFF1/MbbfdxpIlS3j99deZMmUK48eP59e//jU33ngjV155JQcOHKC6upry8nLm\nzZvHX/7yF/Lz85k2bRpz585l8uTJaW2TyJEIm+gdWGxmDvzG3WcDXZLJ2923m1nnoG534J2UZSuC\nMiV6aVN69+7NiBEjAGjfvj3nn38+AEVFRRQUFJCfn09RURFbtmwB4Oyzz+buu++moqKCL33pS/Tv\n35+lS5eycuVKhg0bBsC//vUvOnfu3OD2RLIlbKIf6e7bgmS+xMyaGoJZA2V+SCWzqSRO7dCrV6+Q\nYUjUhBl5Z0rq5Ynz8/MxS+y6sViMgoKCmumDBw8CMGnSJM466yyee+45xo0bx29/+1vcnSlTpnDP\nPfe0fgNEQgp1jt7dtwWPlcAfgOHAe2bWDSB4rAyqVwA9UxbvAWxrYJ2z3b3E3UsKCwsPvwUirWTz\n5s3069ePb33rW4wfP541a9YwduxYFixYQGVlYvfftWsXb731VpYjFamr2URvZsea2XHJaeA8YB2w\nEJgSVJsCPBtMLwQmB9++GQHs0fl5iYJ58+YxaNAgiouL2bBhA5MnT+b000/nrrvu4rzzzmPw4MF8\n7nOfY/t27e7StljiM9MmKpj1IzGKh8Spnifd/W4z6wTMB3oBbwOXuvsuS7z//RVwPvAhcLW7lzW1\njZKSEi8ra7KKREh5eTkDBw7MdhiRo37NPWa2MuWbkI1q9hy9u28GzmygfCcwtoFyB64PGaeIiGSY\nbjwikgFnnXUWH330UZ2yxx9/nKKioixFJLlMiV4kA1599dVshyBSQ9e6ERGJOCV6EZGIU6IXEYk4\nJXoRkYhTohcRiTglepEQOnbs2OhzW7ZsYdCgQa0YjUjLKNGLiEScvkcvWfXuT37CR+XpvR59wcAB\ndL311ibrTJ8+nd69ezNt2jQA7rjjDsyM5cuX8/777/Pxxx9z1113MWHChBZte//+/Xzzm9+krKyM\ndu3a8Ytf/IIxY8awfv16rr76ag4cOEA8Hufpp5/mlFNO4bLLLqOiooLq6mp++MMfcvnllx92u0Ua\no0QvOWnixIl8+9vfrkn08+fP5/nnn+emm27i+OOPZ8eOHYwYMYLx48fXXL44jJkzZwKwdu1aNmzY\nwHnnncebb77Z4E1LFi1axCmnnMJzzz0HwJ49e9LfUBGU6CXLmht5Z8qQIUOorKxk27ZtVFVVceKJ\nJ9KtWzduuukmli9fTiwWY+vWrbz33nt07do19HpXrFjBDTfcAMCAAQPo3bs3b775ZoM3LSkqKuJ7\n3/se06dP56KLLuIzn/lMpporOU7n6CVnXXLJJSxYsIB58+YxceJE5s6dS1VVFStXrmTVqlV06dKF\n/fv3t2idjV0NdtKkSSxcuJBPfOITjBs3jhdffJHTTjuNlStXUlRUxC233MKPf/zjdDRL5BAa0UvO\nmjhxItdeey07duzg5ZdfZv78+XTu3Jn8/Hxeeumlw7qByKhRo5g7dy7nnHMOb775Jm+//Taf/OQn\n69y0ZPPmzaxZs4YBAwZw0kkn8ZWvfIWOHTvy6KOPpr+RIijRSw4744wz2Lt3L927d6dbt25ceeWV\nfOELX6CkpITi4mIGDBjQ4nVOmzaNb3zjGxQVFdGuXTseffRRCgoKmDdvHk888QT5+fl07dqVH/3o\nR7z22mvcfPPNxGIx8vPzmTVrVgZaKRLixiOtQTceyS26QUZmqF9zT9gbj+gcvYhIxOnUjUhIa9eu\n5aqrrqpTVlBQoGvPS5unRC8SUlFREatWrcp2GCItplM3IiIRp0QvIhJxSvQiIhGnRC8SQlOXKRZp\n65ToRdKouro62yGIHCJ0ojezPDP7u5n9MZjva2avmtlGM5tnZu2D8oJgflPwfJ/MhC5y+KZPn86D\nDz5YM3/HHXdw5513MnbsWIYOHUpRURHPPvtsqHUtW7aMMWPGMGnSJIqKitiyZQsDBgzg61//OoMG\nDeLKK6/khRdeYOTIkfTv35/S0lIAXn75ZYqLiykuLmbIkCHs3bsXgPvuu49hw4YxePBgbr/99vQ3\nXnJOS75eeSNQDhwfzN8L/NLdnzKzXwPXALOCx/fd/d/NbGJQTxfZlgb9z/w32fHOvrSu8+SeHfnM\nZac1WSfdlykuLS1l3bp19O3bly1btrBp0yZ+97vfMXv2bIYNG8aTTz7JihUrWLhwIT/5yU945pln\n+PnPf87MmTMZOXIk+/bto0OHDixevJiNGzdSWlqKuzN+/HiWL1/OqFGj0tI3kptCjejNrAfweeC3\nwbwB5wALgipzgC8G0xOCeYLnx1pLLugt0gpSL1O8evXqmssU33rrrQwePJhzzz235jLFYQwfPpy+\nffvWzPft25eioiJisRhnnHEGY8eOxcxqRvwAI0eO5Dvf+Q4PPPAAu3fvpl27dixevJjFixczZMgQ\nhg4dyoYNG9i4cWMmukBySNgR/f3A94HjgvlOwG53PxjMVwDdg+nuwDsA7n7QzPYE9XekrtDMpgJT\nAXr16nW48ctRrrmRdyYlL1P87rvvHnKZ4vz8fPr06RP6MsXHHntsnfmCgoKa6VgsVjMfi8U4eDBx\n2MyYMYPPf/7zLFq0iBEjRvDCCy/g7txyyy1cd911aWqlSIgRvZldBFS6+8rU4gaqeojnagvcZ7t7\nibuXFBYWhgpWJJ0mTpzIU089xYIFC7jkkkvYs2fPEV+muCX+8Y9/UFRUxPTp0ykpKWHDhg2MGzeO\nhx9+mH37Eqeztm7dSmVlZUbjkOgLM6IfCYw3swuBDiTO0d8PnGBm7YJRfQ9gW1C/AugJVJhZO+Df\ngF1pj1zkCGXiMsUtcf/99/PSSy+Rl5fH6aefzgUXXEBBQQHl5eWcffbZQOJrnU888QSdO3fOaCwS\nbS26TLGZjQa+5+4XmdnvgKdTPoxd4+4Pmtn1QJG7fyP4MPZL7n5ZU+vVZYpziy6nmxnq19zTGpcp\nng58x8w2kTgH/1BQ/hDQKSj/DjDjCLYhIiJHqEVXr3T3ZcCyYHozMLyBOvuBS9MQm0ibkq3LFO/c\nuZOxY8ceUr506VI6deqU0W1LNOgyxSIhZesyxZ06ddLlkeWI6BIIkhVt4RaWUaL+lKYo0Uur69Ch\nAzt37lRyShN3Z+fOnXTo0CHboUgbpVM30up69OhBRUUFVVVV2Q4lMjp06ECPHj2yHYa0UUr00ury\n8/PrXC5ARDJLp25ERCJOiV5EJOKU6EVEIk6JXkQk4pToRUQiToleRCTilOhFRCJOiV5EJOKU6EVE\nIk6JXkQk4pToRUQiToleRCTilOhFRCKuTVy9cs+/PubZVVtxh7g78eDR3TEMM4iZEYslHs0MC5Y1\no6YOUFNeX/LK58lLoDtO/cuhp67LkvPBthLbTTxitdupWUWIS6t7I5XcqWm7k7i+eNxrY41Zw7Gk\n9lN1PNl3jpkRS/ZZsFwsWDZ1G4ltJuIygmViVrNcsq3uTjyeaGJye+61MaXWT5Y1pqZfk/0cTDsO\nXtumZJyHdFnqqr3u6+heW92AWIw6+0/9PovHa/c38KAdif0stV3J9caDieQ24zXbTfZpbcBmRp4Z\neTEjFktMx4JhVTzltYqn9G3tPpfaP1ZTnmhX7f4Jta9fYn859LWNB9PJfSrRF4n+qBObWU27PKVN\nTe3XzqF9mNxmcn+qsy8Gr0f9Yzy1L6nZP2qnoe7+lbqfNbSn1R7rddtQ//irWbrePpXavsR66u5b\nNeuts67a/b5u7HVfu9R+Sz3WU/fcZPtr2wnVcaiOJ/aX5GO8BfdzaBOJ/u1dH3LjU7pVmohIJrSJ\nRH9a5+N45rufbXRkGI97zYg39T9Z7egjsZ7kiKSxAWXyP2rq6D85nbqumpFNvenkKDM5Sqr/LqKp\nkWxtDA1L/e9dM33ICKBuLHmxlHc7ZuTFarfgKaOmuNf2X+q6U0e8UDtiqP/OKq/eSCr5WNMnKfWr\n402NMrxuP6f0Z913FIl2xKxun6bekap2BHzoiDfZZw29S4ql9Fedd2nUb0vq6DTYjtXdZnL5mtc2\n5TVLrqN2FJbom+T28lLeNeTV708OHZ0n25zst+R87bu2un0WS2lX6mg69XWujjvVKe8IU9/Jpo4s\nm9qvk7Gn7ofJ6sl3K/X7NS9mje6HyWMOao/r2ncl9Y/Bpva12mO9odF26ug8Od/QcZw6Ok89VpJl\nqXGmTtdvQ+r2Utue+g4uuVzqO9Rke5PvCPNq3h0mHvve20QXpGgTib4gP8aphR2zHYaISCTpw1gR\nkYhrNtGbWQczKzWz1Wa23szuDMr7mtmrZrbRzOaZWfugvCCY3xQ83yezTRARkaaEGdF/BJzj7mcC\nxcD5ZjYCuBf4pbv3B94HrgnqXwO87+7/DvwyqCciIlnSbKL3hH3BbH7w58A5wIKgfA7wxWB6QjBP\n8PxYC/MppYiIZESoc/Rmlmdmq4BKYAnwD2C3ux8MqlQA3YPp7sA7AMHze4BO6QxaRETCC5Xo3b3a\n3YuBHsBwYGBD1YLHpn7DUMPMpppZmZmVVVVVhY1XRERaqEXfunH33cAyYARwgpklv57ZA9gWTFcA\nPQGC5/8N2NXAuma7e4m7lxQWFh5e9CIi0qww37opNLMTgulPAOcC5cBLwCVBtSnAs8H0wmCe4PkX\nvelfNoiISAaF+cFUN2COmeWR+Mcw393/aGavA0+Z2V3A34GHgvoPAY+b2SYSI/mJGYhbRERCajbR\nu/saYEgD5ZtJnK+vX74fuDQt0YmIyBHTL2NFRCJOiV5EJOKU6EVEIk6JXkQk4pToRUQiToleRCTi\nlOhFRCJOiV5EJOKU6EVEIk6JXkQk4pToRUQiToleRCTilOhFRCJOiV5EJOKU6EVEIk6JXkQk4pTo\nRUQiToleRCTilOhFRCJOiV5EJOKU6EVEIk6JXkQk4pToRUQiToleRCTimk30ZtbTzF4ys3IzW29m\nNwblJ5nZEjPbGDyeGJSbmT1gZpvMbI2ZDc10I0REpHFhRvQHge+6+0BgBHC9mZ0OzACWunt/YGkw\nD3AB0D/4mwrMSnvUIiISWrOJ3t23u/vfgum9QDnQHZgAzAmqzQG+GExPAB7zhFeAE8ysW9ojFxGR\nUFp0jt7M+gBDgFeBLu6+HRL/DIDOQbXuwDspi1UEZSIikgWhE72ZdQSeBr7t7h80VbWBMm9gfVPN\nrMzMyqqqqsKGISIiLRQq0ZtZPokkP9fdfx8Uv5c8JRM8VgblFUDPlMV7ANvqr9PdZ7t7ibuXFBYW\nHm78IiLSjDDfujHgIaDc3X+R8tRCYEowPQV4NqV8cvDtmxHAnuQpHhERaX3tQtQZCVwFrDWzVUHZ\nrcBPgflmdg3wNnBp8Nwi4EJgE/AhcHVaIxYRkRZpNtG7+woaPu8OMLaB+g5cf4RxiYhImuiXsSIi\nEadELyIScUr0IiIRp0QvIhJxSvQiIhGnRC8iEnFK9CIiEadELyIScUr0IiIRp0QvIhJxSvQiIhGn\nRC8iEnFK9CIiEadELyIScUr0IiIRp0QvIhJxSvQiIhGnRC8iEnFK9CIiEadELyIScUr0IiIRp0Qv\nIhJxSvQiIhGnRC8iEnHNJnoze9jMKs1sXUrZSWa2xMw2Bo8nBuVmZg+Y2SYzW2NmQzMZvIiINC/M\niP5R4Px6ZTOApe7eH1gazANcAPQP/qYCs9ITpoiIHK5mE727Lwd21SueAMwJpucAX0wpf8wTXgFO\nMLNu6QpWRERa7nDP0Xdx9+0AwWPnoLw78E5KvYqg7BBmNtXMysysrKqq6jDDEBGR5qT7w1hroMwb\nqujus929xN1LCgsL0xyGiIgkHW6ify95SiZ4rAzKK4CeKfV6ANsOPzwRETlSh5voFwJTgukpwLMp\n5ZODb9+MAPYkT/GIiEh2tGuugpn9FzAaONnMKoDbgZ8C883sGuBt4NKg+iLgQmAT8CFwdQZiFhGR\nFmg20bv7FY08NbaBug5cf6RBiYhI+uiXsSIiEadELyIScUr0IiIRp0QvIhJxSvQiIhGnRC8iEnFK\n9CIiEadELyIScUr0IiIRp0QvIhJxSvQiIhGnRC8iEnFK9CIiEadELyIScUr0IiIRp0QvIhJxSvQi\nIhGnRC8iEnHN3kqwNeza9k/m3v5K2tZnlrZViYgc9dpEom/XPsbJPTumZ2WentWIiERFm0j0ewp2\nsKjf/2vRMnaUDtuNQ+P2iP13aqiNhytqfZNuR9v+5H54sWXzeA8bc/0Y29Jr0yYS/YHqA1Tsqwhd\nvy3vyE1paodpjR3Z3Vu8nZYuc7gHclPS1Tet0f7WWldyfY1pywOhlg4E0nm8H+5r0FzM9WNM52uT\njv2mTST6U084lafHP53tMEREjiph/2lm5Fs3Zna+mb1hZpvMbEYmtiEiIuGkPdGbWR4wE7gAOB24\nwsxOT/d2REQknEyM6IcDm9x9s7sfAJ4CJmRgOyIiEkImztF3B95Jma8AzmpyicpymNl0FREROTyZ\nSPQNfTpwyEfQZjYVmAowqHtHKPxkBkIREYmy0lC1MpHoK4CeKfM9gG31K7n7bGA2QElJiXPZYxkI\nRUQkwi5/PFS1TJyjfw3ob2Z9zaw9MBFYmIHtiIhICGkf0bv7QTP7D+DPQB7wsLuvT/d2REQknIz8\nYMrdFwGLMrFuERFpGV2mWEQk4pToRUQiToleRCTilOhFRCJOiV5EJOIsE9cPb3EQZnuBN7IdRxad\nDOzIdhBZlut9kOvtB/XB4bS/t7sXNlepTVyPHnjD3UuyHUS2mFlZLrcf1Ae53n5QH2Sy/Tp1IyIS\ncUr0IiIR11YS/exsB5Blud5+UB/kevtBfZCx9reJD2NFRCRz2sqIXkREMiTriT7XbiRuZg+bWaWZ\nrUspO8nMlpjZxuDxxGzGmElm1tPMXjKzcjNbb2Y3BuW51AcdzKzUzFYHfXBnUN7XzF4N+mBecJnv\nyDKzPDP7u5n9MZjPmfab2RYzW2tmq8ysLCjL2DGQ1USfozcSfxQ4v17ZDGCpu/cHlgbzUXUQ+K67\nDwRGANcHr3ku9cFHwDnufiZQDJxvZiOAe4FfBn3wPnBNFmNsDTcC5Snzudb+Me5enPKVyowdA9ke\n0efcjcTdfTmwq17xBGBOMD0H+GKrBtWK3H27u/8tmN5L4kDvTm71gbv7vmA2P/hz4BxgQVAe6T4w\nsx7A54HfBvNGDrW/ERk7BrKd6Bu6kXj3LMWSTV3cfTskEiHQOcvxtAoz6wMMAV4lx/ogOG2xCqgE\nlgD/AHa7+8GgStSPhfuB7wPxYL4TudV+Bxab2crg/tmQwWMg27+MDXUjcYkeM+sIPA18290/SAzo\ncoe7VwPFZnYC8AdgYEPVWjeq1mFmFwGV7r7SzEYnixuoGsn2B0a6+zYz6wwsMbMNmdxYtkf0oW4k\nngPeM7NuAMFjZZbjySgzyyeR5Oe6+++D4pzqgyR33w0sI/F5xQlmlhx8RflYGAmMN7MtJE7XnkNi\nhJ8r7cfdtwWPlST+0Q8ng8dAthO9biSesBCYEkxPAZ7NYiwZFZyLfQgod/dfpDyVS31QGIzkMbNP\nAOeS+KziJeCSoFpk+8Ddb3H3Hu7eh8Qx/6K7X0mOtN/MjjWz45LTwHnAOjJ4DGT9B1NmdiGJ/+bJ\nG4nfndWAMszM/gsYTeJKde8BtwPPAPOBXsDbwKXuXv8D20gws/8D/A+wltrzs7eSOE+fK30wmMSH\nbXkkBlvz3f3HZtaPxAj3JODvwFfc/aPsRZp5wamb77n7RbnS/qCdfwhm2wFPuvvdZtaJDB0DWU/0\nIiKSWdk+dSMiIhmmRC8iEnFK9CIiEadELyIScUr0IiIRp0QvIhJxSvQiIhGnRC8iEnH/H/yX5qlw\n40rHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2dcfb96828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('mlp_model_OS.h5', custom_objects={'rmse_': rmse_})\n",
    "model2 = load_model('mlp_model_SMOTE.h5', custom_objects={'rmse_': rmse_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets test our models on the spenders in testing set\n",
    "spender_test_index = y_test>0\n",
    "\n",
    "pred_os = model.predict(norm_test_x[spender_test_index])\n",
    "pred_smote = model2.predict(dim_reduced_test_x[spender_test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAs8AAAFRCAYAAACL9kAZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nasngWZIkSWrp/wPUgdTI19I/6AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f98f6cc4fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, (ax1,ax2) = plt.subplots(1, 2, figsize=(12,5))\n",
    "\n",
    "ax1.hist(pred_os)\n",
    "ax1.set_title('Distribution of prediction using oversampled data', fontsize=14)\n",
    "ax1.set_xlabel('LTV', fontsize=13)\n",
    "ax2.hist(pred_smote)\n",
    "ax2.set_title('Distribution of prediction using Smote data', fontsize=14)\n",
    "ax2.set_xlabel('LTV', fontsize=13)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nrmse for all spenders using mlp trained on oversampled data:  1.4888\n",
      "nrmse for all spenders using mlp trained on smote data:  1.5169\n"
     ]
    }
   ],
   "source": [
    "mean_ltv = y_test[spender_test_index].mean()\n",
    "\n",
    "\n",
    "nrmse_os_all = rmse(y_test[spender_test_index].reshape((pred_os.shape)), pred_os)/mean_ltv\n",
    "nrmse_smote_all = rmse(y_test[spender_test_index].reshape((pred_smote.shape)), pred_smote)/mean_ltv\n",
    "print('nrmse for all spenders using mlp trained on oversampled data: ', '%.4f'%nrmse_os_all)\n",
    "print('nrmse for all spenders using mlp trained on smote data: ', '%.4f'%nrmse_smote_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nrmse for the premium users using mlp trained on oversampled data:  1.9919\n",
      "nrmse for the premium users using mlp trained on smote data:  2.4049\n"
     ]
    }
   ],
   "source": [
    "premium_index = np.where(np.logical_and(y_test>30, y_test<=70))[0]\n",
    "\n",
    "pred_premium_os = model.predict(norm_test_x[premium_index])\n",
    "pred_premium_smote = model2.predict(dim_reduced_test_x[premium_index])\n",
    "\n",
    "nrmse_premium_os = rmse(y_test[premium_index].reshape((pred_premium_os.shape)), pred_premium_os)/mean_ltv\n",
    "nrmse_premium_smote = rmse(y_test[premium_index].reshape((pred_premium_smote.shape)), pred_premium_smote)/mean_ltv\n",
    "print('nrmse for the premium users using mlp trained on oversampled data: ', '%.4f'%nrmse_premium_os)\n",
    "print('nrmse for the premium users using mlp trained on smote data: ', '%.4f'%nrmse_premium_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nrmse for the HIGH SPENDERS using mlp trained on oversampled data:  5.8870\n",
      "nrmse for the HIGH SPENDERS using mlp trained on smote data:  6.4495\n"
     ]
    }
   ],
   "source": [
    "pred_high_os = model.predict(norm_test_x[y_test>100])\n",
    "pred_high_smote = model2.predict(dim_reduced_test_x[y_test>100])\n",
    "\n",
    "nrmse_high_os = rmse(y_test[y_test>100].reshape((pred_high_os.shape)), pred_high_os)/mean_ltv\n",
    "nrmse_high_smote = rmse(y_test[y_test>100].reshape((pred_high_smote.shape)), pred_high_smote)/mean_ltv\n",
    "print('nrmse for the HIGH SPENDERS using mlp trained on oversampled data: ', '%.4f'%nrmse_high_os)\n",
    "print('nrmse for the HIGH SPENDERS using mlp trained on smote data: ', '%.4f'%nrmse_high_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tune_model = KerasRegressor(build_fn=create_model, \n",
    "#                             verbose=0,\n",
    "#                             epochs=500, \n",
    "#                             learn_rate=0.01, \n",
    "#                             decay=0.01, \n",
    "#                             init='normal', \n",
    "#                             drop_out=0.2, \n",
    "#                             loss='mae', \n",
    "#                             units=32)\n",
    "\n",
    "# batches = [300, 400]\n",
    "# decay = [0.1, 0.001, 0.0001]\n",
    "\n",
    "# param_grid = dict(\n",
    "#                   decay=decay,\n",
    "#                   batch_size=batches, \n",
    "#                   )\n",
    "# param_grid\n",
    "\n",
    "# grid = GridSearchCV(estimator=tune_model, param_grid=param_grid, cv=5, n_jobs=30, verbose=1)\n",
    "# grid_result = grid.fit(res_x,res_y)\n",
    "\n",
    "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "# means = grid_result.cv_results_['mean_test_score']\n",
    "# stds = grid_result.cv_results_['std_test_score']\n",
    "# params = grid_result.cv_results_['params']\n",
    "# for mean, stdev, param in zip(means, stds, params):\n",
    "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
